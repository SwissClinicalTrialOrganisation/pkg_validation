[
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "Writing function tests",
    "section": "",
    "text": "Note\n\n\n\nThis page is adapted from a vignette in the validation package\nThe validation package contains a set of functions to assist in the running and reporting of tests that have been published on the SCTO platform. The tests themselves are located in the validation_tests repository. The results of the tests are then published as issues in the pkg_validation repository.\nTests are written using functions from the testthat package, and can be downloaded, run, and reported using the functions in the validation package.",
    "crumbs": [
      "Home",
      "Documentation",
      "Writing function tests"
    ]
  },
  {
    "objectID": "testing.html#core-principles",
    "href": "testing.html#core-principles",
    "title": "Writing function tests",
    "section": "Core principles",
    "text": "Core principles\n\nthe unit of testing is the function, not the package\nit is not necessary to test all functions, only those that are used in the product\n\ne.g. assuming that the lme4 package is classified as being high risk, it may be that initially only lmer is tested.\nSome time later, glmer is used in a product, and so it is necessary to test this function as well.\n\ntests should be written in a way that they can be run automatically\ntests are written using the testthat package\ntests are documented sufficiently to allow others to understand what is being tested and why (e.g. the documentation establishes the user requirements)\n\nthis principle is not thoroughly applied in this document as it is informative only. See existing examples in the repository.",
    "crumbs": [
      "Home",
      "Documentation",
      "Writing function tests"
    ]
  },
  {
    "objectID": "testing.html#test_skeleton-helps-build-the-structure",
    "href": "testing.html#test_skeleton-helps-build-the-structure",
    "title": "Writing function tests",
    "section": "test_skeleton helps build the structure",
    "text": "test_skeleton helps build the structure\nThe test_skeleton function can be used to create the relevant folder structure for testing a new package, or adding a file for testing additional functions. In the code below, substitute pkg with the name of the package to be tested and add the names of the function(s) you want to test in the funs argument.\nThis will create a set of files in your working directory:\n-- pkg\n   +- info.txt\n   +- setup-pkg.R\n   +- test-fun.R\n   +- test-fun2.R\n   +- test-etc.R\n\ninfo.txt file will contain the name of the package and a freetext description of what is tested,\nsetup-pkg.R is for any necessary setup code (e.g. installation of the package),\nfor each function in the funs argument, a test-function.R file is created, which will contain the actual testing code.\n\nIn the event that the package already has tests, the test_skeleton function will not overwrite the existing files, only adding any necessary test-function.R files.\nAdd the relevant tests to the test-function.R files and check that they work as expected (run devtools::load_all() followed by test(\"package\")).",
    "crumbs": [
      "Home",
      "Documentation",
      "Writing function tests"
    ]
  },
  {
    "objectID": "testing.html#writing-tests",
    "href": "testing.html#writing-tests",
    "title": "Writing function tests",
    "section": "Writing tests",
    "text": "Writing tests\nTesting is performed via the testthat framework. All tests for a given function should be placed in a dedicated test-function.R file.\nEach test is comprised of one or more expectations and a descriptive name.\nE.g.\nWhere multiple tests are to be made on what could be a single object, it is often useful to create the object outside of the test_that function. This is particularly useful when different descriptive texts should be shown for the tests (e.g. perhaps the coefficients and standard errors from a model):\nIf objects are only useful to the test, they can be created within the test_that function.\nMaking the description of the test meaningful is important, as it will help the user diagnose where the problem is.\ntestthat supports a large number of expectations, which are documented in the testthat documentation. We demonstrate a few examples below.\n\nCompare computation to a reference value\nTo test the computation of the function, the following code must be added to the testing file, for as many test cases as considered appropriate:\nWhere f is the function to be tested, x are the input parameters for the function and y is the expected returned value.\nNote that is/may be necessary/desirable to set a tolerance for floating point comparisons. This can be done with the tolerance argument.\n\n\nTesting for errors, warnings and other messages\nTo test whether, under certain conditions, the function returns an error, a warning or a message, the following corresponding code can be adapted, for as many test cases as considered appropriate:\nWhere f is the function to be tested, x are the arguments that define the conditions. Use the regexp argument to check for a particular error, warning or message.\nIn contrast, to test whether the function runs without returning an error, a warning or a message, the following corresponding code can be adapted, for as many test cases as considered appropriate:\n\n\nTesting booleans\nTo test whether, under certain conditions, the function returns TRUE or FALSE, adapt the following code as appropriate:\nWhere f is the function to be tested, x are the arguments that define the conditions.\n\n\nTesting for NULL\nTo test whether, under certain conditions, the function returns NULL, the following code can be adapted, for as many test cases as considered appropriate:\n\n\nTesting the type of object returned (base R)\nTo test whether, the function returns an object of a certain type, the following code can be adapted, for as many test cases as considered appropriate:\nWhere f is the function to be tested, x are the arguments that define the conditions and type is any of the following: “integer”, “character”, “factor”, “logical”, “double”.\n\n\nTesting the class (s3) of an object\nTo test whether, the function returns an object of class s3, the following code can be adapted, for as many cases as considered appropriate:\nWhere f is the function to be tested, x are the arguments that define the conditions and class is, among others, any of the following: “data.frame”, “factor”, “Date”, “POSIXct”, etc.\n\n\nRunning tests under certain conditions\nOn occasion, it may be desirable to restrict the tests to specific package versions. This can be done by using the skip_if functions in testthat.\nFor example, the pivot functions we introduced to tidyr in version 1.0.0. If we have tests on those functions, we can restrict them to versions 1.0.0 and above with the following code:\nskip_if(packageVersion(\"tidyr\") &lt; \"1.0.0\")\nThis line can be placed at the top of the test file, before any tests are run. The equivalent can be done for versions below a certain version, which might be useful for deprecated functions.\nIt might be suitable to stop tests from being run if the internet is not available:\nskip_if_offline()\nOr if a package can only be run on a specific operating system:\nskip_on_os(\"mac\")",
    "crumbs": [
      "Home",
      "Documentation",
      "Writing function tests"
    ]
  },
  {
    "objectID": "testing.html#submitting-tests-to-the-platform",
    "href": "testing.html#submitting-tests-to-the-platform",
    "title": "Writing function tests",
    "section": "Submitting tests to the platform",
    "text": "Submitting tests to the platform\nOnce you have added the necessary tests, add the files to the validation_tests repository. The easiest way is to navigate to the tests folder, click on “Add file” and then “Upload files”. Now you can simply drag the pkg folder into the browser window and commit the change. This will have forked the repository to your own GitHub account, and you can now create a pull request to the original repository to incorporate your code. It is also possible to fork the repository, clone it to your computer and make the commit there, but this is not strictly necessary.\nAt this stage, the four-eye principle will be applied to the pull request to check the adequacy and quality of the tests and code. If the reviewer agrees with your tests, they will be merged into the package. If they note any issues, which you will see as comments in the GitHub pull request, you will need to address them before the tests can be merged.\nOnce merged, the tests can be run via the validation package validation::test(\"packagename\") and documented in the repository at https://github.com/SwissClinicalTrialOrganisation/pkg_validation.",
    "crumbs": [
      "Home",
      "Documentation",
      "Writing function tests"
    ]
  },
  {
    "objectID": "testing.html#worked-example",
    "href": "testing.html#worked-example",
    "title": "Writing function tests",
    "section": "Worked example",
    "text": "Worked example\nTheory is all well and good, but it’s always useful to see how that would be in practice.\nAssume that we want to check that the lm function from the stats package works as expected. We can write a test file that checks that the function returns the expected coefficients and standard errors.\nThe following assumes that we have cloned the validation repository to our computer and we are within that project.\nTo begin with, we construct the testing files, specifically the test_skeleton function. We only want to test the lm function, so we only pass that to the second argument:\nThis will have created a stats folder within inst/tests. Within that folder, there will be files called info.txt, setup-stats.R, and test-lm.R.\n\ntest-lm.R\nFirst we will write the actual tests that we want to run. Tests are entered into the test-lm.R file. Opening that file, we see that there are just a few comments at the top of the file, with some reminders.\n# Write relevent tests for the function in here\n# Consider the type of function:\n#   - is it deterministic or statistic?\n#   - is it worth checking for errors/warnings under particular conditions?\nWe decide that we will use the mtcars dataset as our basis for testing lm, so we can load the dataset. We want to test both the linear effect of the number of cylinders on the miles per gallon, and the effect of the number of cylinders (cyl), as well as when cyl is treated as a factor.\n# Write relevent tests for the function in here\n# Consider the type of function:\n#   - is it deterministic or statistic?\n#   - is it worth checking for errors/warnings under particular conditions?\n\ndata(mtcars)\nmtcars$cyl_f &lt;- factor(mtcars$cyl)\nNote that if there are multiple functions being tested (each in their own test-function.R file) that require the same data, we can load and prepare the data in the setup-stats.R file.\nWe can also define the models that we want to test:\ncmod &lt;- lm(mpg ~ cyl, data = mtcars)\nfmod &lt;- lm(mpg ~ cyl_f, data = mtcars)\nWe do not include the model definitions within a test_that call because we will use the same models in multiple tests. Again, if we needed to use those models for testing multiple functions, we could define them in the setup file.\n\nTesting coefficients\nSuppose that we know that the coefficient for mpg ~ cyl is known (-2.88 for the linear effect). We can write a test that checks that expectation:\ntest_that(\"lm returns the expected coefficients\", {\n  expect_equal(coef(cmod)[2], -2.88)\n})\nDue to floating point precision, this is probably insufficient - R will not return exactly -2.88. We can use the tolerance argument to check that the coefficient is within a certain range (we could also round the coefficient). We also need to tell expect_equal to ignore the names attribute of the vector, otherwise it compares the whole object, attributes and all:\ntest_that(\"lm returns the expected coefficients\", {\n  expect_equal(coef(cmod)[2], -2.88, tolerance = 0.01, check.attributes = FALSE)\n})\nWe can do the same for the coefficients from the model with cyl_f. This time, we can derive the values from the tapply function as, in this case, the coefficients are just the means:\ntest_that(\"lm returns the expected coefficients\", {\n  means &lt;- tapply(mtcars$mpg, mtcars$cyl, mean)\n  coefs &lt;- coef(fmod)\n  expect_equal(coefs[1], means[1], check.attributes = FALSE)\n  expect_equal(coefs[2], means[2] - means[1], check.attributes = FALSE)\n  expect_equal(coefs[3], means[3] - means[1], check.attributes = FALSE)\n})\nWe have now performed 4 tests (the expectations) in two test_that calls. We can also combine them together:\ntest_that(\"lm returns the expected coefficients\", {\n  expect_equal(coef(cmod)[2], -2.88, tolerance = 0.01, check.attributes = FALSE)\n  \n  means &lt;- tapply(mtcars$mpg, mtcars$cyl, mean)\n  coefs &lt;- coef(fmod)\n  expect_equal(coefs[1], means[1], check.attributes = FALSE)\n  expect_equal(coefs[2], means[2] - means[1], check.attributes = FALSE)\n  expect_equal(coefs[3], means[3] - means[1], check.attributes = FALSE)\n})\nWhether to put them in one or two calls is up to the author. Distributing them across more calls helps identify which tests fail, but it also makes the file longer.\n\nA note on selecting tolerances\nThe tolerance is a tricky thing to select. It is a balance between being too strict and too lenient. If the tolerance is too strict, then the test will fail when the function is working as expected. If the tolerance is too lenient, then the test will pass when the function is not working as expected.\nConsider the example above. We compared -2.88 with the coefficient which R reports to (at least) 5 decimal places. In this case, it does not make sense to use a tolerance of less than 0.01 because we only know the coefficient to two decimal places (even though we would have access to a far greater precision had we worked for it).\nGenerally speaking, values that are easy to calculate should probably have a lower tolerance. Values that are very dependent on specifics of the implementation (e.g. maximisation algorithm, etc) should probably have a higher tolerance. This is especially the case when using external software as a reference (e.g. Stata uses different defaults settings to lme4, causing differences in SEs). Simulation results, may also require a more lenient tolerance.\n\n\n\nTesting the standard errors against Stata\nSuppose that we have used Stata as a reference software for the standard errors. We include the commands used in the reference software in comments in the script, including the output and the information of the version of the reference software.\n# write.csv(mtcars, \"mtcars.csv\", row.names = FALSE)\n# reference software: Stata 17.0 (revision 2024-02-13)\n# import delimited \"mtcars.csv\"\n# regress mpg cyl\n# [output truncated for brevity]\n# ------------------------------------------------------------------------------\n#          mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n# -------------+----------------------------------------------------------------\n#          cyl |   -2.87579   .3224089    -8.92   0.000    -3.534237   -2.217343\n#        _cons |   37.88458   2.073844    18.27   0.000     33.64922    42.11993\n# ------------------------------------------------------------------------------\n# [output truncated for brevity]\n# regress mpg i.cyl\n# [output truncated for brevity]\nWe can then use the SE values from Stata in the tests, specifying a suitable tolerance (it’s pretty simple to calculate, so we can be quite stringent):\ntest_that(\"Standard errors from LM are correct\", {\n  expect_equal(summary(cmod)$coefficients[2, 2], 0.322408, \n               tolerance = 0.00001)\n  expect_equal(summary(fmod)$coefficients[2, 2], 1.558348, \n               tolerance = 0.00001)\n  expect_equal(summary(fmod)$coefficients[3, 2], 1.298623, \n               tolerance = 0.0001)\n})\n\n\n\nThe completed test file\nThe test file including the tests above is then:\n# Write relevent tests for the function in here\n# Consider the type of function:\n#   - is it deterministic or statistic?\n#   - is it worth checking for errors/warnings under particular conditions?\n\ndata(mtcars)\nmtcars$cyl_f &lt;- factor(mtcars$cyl)\n\ncmod &lt;- lm(mpg ~ cyl, data = mtcars)\nfmod &lt;- lm(mpg ~ cyl_f, data = mtcars)\n\ntest_that(\"lm returns the expected coefficients\", {\n  expect_equal(coef(cmod)[2], -2.88, tolerance = 0.01, check.attributes = FALSE)\n  \n  means &lt;- tapply(mtcars$mpg, mtcars$cyl, mean)\n  coefs &lt;- coef(fmod)\n  expect_equal(coefs[1], means[1], check.attributes = FALSE)\n  expect_equal(coefs[2], means[2] - means[1], check.attributes = FALSE)\n  expect_equal(coefs[3], means[3] - means[1], check.attributes = FALSE)\n})\n\n# write.csv(mtcars, \"mtcars.csv\", row.names = FALSE)\n# reference software: Stata 17.0 (revision 2024-02-13)\n# import delimited \"mtcars.csv\"\n# regress mpg cyl\n# \n#       Source |       SS           df       MS      Number of obs   =        32\n# -------------+----------------------------------   F(1, 30)        =     79.56\n#        Model |  817.712952         1  817.712952   Prob &gt; F        =    0.0000\n#     Residual |  308.334235        30  10.2778078   R-squared       =    0.7262\n# -------------+----------------------------------   Adj R-squared   =    0.7171\n#        Total |  1126.04719        31  36.3241028   Root MSE        =    3.2059\n# \n# ------------------------------------------------------------------------------\n#          mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n# -------------+----------------------------------------------------------------\n#          cyl |   -2.87579   .3224089    -8.92   0.000    -3.534237   -2.217343\n#        _cons |   37.88458   2.073844    18.27   0.000     33.64922    42.11993\n# ------------------------------------------------------------------------------\n# regress mpg i.cyl\n# \n#       Source |       SS           df       MS      Number of obs   =        32\n# -------------+----------------------------------   F(2, 29)        =     39.70\n#        Model |   824.78459         2  412.392295   Prob &gt; F        =    0.0000\n#     Residual |  301.262597        29  10.3883654   R-squared       =    0.7325\n# -------------+----------------------------------   Adj R-squared   =    0.7140\n#        Total |  1126.04719        31  36.3241028   Root MSE        =    3.2231\n# \n# ------------------------------------------------------------------------------\n#          mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n# -------------+----------------------------------------------------------------\n#          cyl |\n#           6  |  -6.920779   1.558348    -4.44   0.000    -10.10796   -3.733599\n#           8  |  -11.56364   1.298623    -8.90   0.000    -14.21962   -8.907653\n#              |\n#        _cons |   26.66364   .9718008    27.44   0.000     24.67608    28.65119\n# ------------------------------------------------------------------------------\n\ntest_that(\"Standard errors from lm are correct\", {\n  expect_equal(summary(cmod)$coefficients[2, 2], 0.322408, \n               tolerance = 0.0001)\n  expect_equal(summary(fmod)$coefficients[2, 2], 1.558348, \n               tolerance = 0.0001)\n  expect_equal(summary(fmod)$coefficients[3, 2], 1.298623, \n               tolerance = 0.0001)\n})\nFor lm, other things that might be tested include the R-squared, the F-statistic, and the p-values. Generally speaking, we might also want to test that the model is of the appropriate class (also lm in this case), or that the model has the expected number of coefficients, that the function issues warnings and/or errors at appropriate times.\n\n\n\ninfo.txt\nIt is easiest to write the info.txt file once all tests have been written. It provides a listing of what has been tested in prose form and serves as a quick overview of the tests.\nThe default text the file contains a single line:\nTests for package stats\nExtra details on the tests that we have performed should be added. In this case, we might modify it to:\nTests for package stats\n- coefficients and SEs from a unvariate model with continuous and factor predictors. SEs were checked against Stata.\nWhere tests are for/from a specific version of the package, as might be the case for newly added or deprecated functions, this should also be noted.\n\n\nsetup-stats.R\nThis file should contain the code necessary to load the package and any other packages that are required for the tests. In this case, we need the stats package and the testthat package.\nThe testthat package is always needed. In general, loading the package is necessary. As stats is a standard R package, it’s no necessary in this case.\nWe also try to leave the environment as we found it, so we detach the packages via the withr::defer function. Again, in this case, we don’t want to detach it, as it is a standard package.\nif(!require(stats)) install.packages(\"stats\")\nlibrary(stats)\nlibrary(testthat)\nwithr::defer({\n  # most of the time, we would want to detach packages, in this case we don't\n  # detach(package:stats)\n}, teardown_env())\n\n\nTesting that the tests work\nAssuming the tests are in a folder called stats, which is within out current working directory, we can run the tests with:\nvalidation::test(\"stats\", download = FALSE)\nWe specify download = FALSE because validation::test will download the files from GitHub and run those tests by default. download = FALSE tells it to use the local copy of the files instead.\nThe output should return various information on our tests and system. The first part comes from testthat itself while it runs the tests, the remainder (after “Copy and paste the following…”) provides summary information that should be copied into a github issue :\n✔ | F W  S  OK | Context\n✔ |          7 | lm                                                                          \n\n══ Results ══════════════════════════════════════════════════════════════════════════════════\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 7 ]\n## Copy and paste the following output into the indicated sections of a new issue\n\nISSUE NAME: \n[Package test]: stats version 4.3.1 \n\n### Name of the package you have validated: \nstats \n\n### What version of the package have you validated? \n4.3.1 \n\n### When was this package tested? \n2024-03-18 \n\n### What was tested? \nTests for package stats \n - coefficients and SEs from a unvariate model with continuous and factor predictors. SEs were checked against Stata. \n \n\n### Test results \nPASS \n\n### Test output:\n\n\n|file      |context |test                                 | nb| passed|skipped |error | warning|\n|:---------|:-------|:------------------------------------|--:|------:|:-------|:-----|-------:|\n|test-lm.R |lm      |lm returns the expected coefficients |  8|      4|FALSE   |FALSE |       4|\n|test-lm.R |lm      |Standard errors from lm are correct  |  3|      3|FALSE   |FALSE |       0|\n\n### SessionInfo:\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=German_Switzerland.utf8  LC_CTYPE=German_Switzerland.utf8   \n[3] LC_MONETARY=German_Switzerland.utf8 LC_NUMERIC=C                       \n[5] LC_TIME=German_Switzerland.utf8    \n\ntime zone: Europe/Zurich\ntzcode source: internal\n\nattached base packages:\n[1] graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] gh_1.4.0         validation_0.1.0 testthat_3.2.0  \n\nloaded via a namespace (and not attached):\n [1] xfun_0.40         httr2_0.2.3       htmlwidgets_1.6.2 devtools_2.4.5   \n [5] remotes_2.4.2.1   processx_3.8.2    callr_3.7.3       vctrs_0.6.5      \n [9] tools_4.3.1       ps_1.7.5          generics_0.1.3    curl_5.1.0       \n[13] tibble_3.2.1      fansi_1.0.6       pkgconfig_2.0.3   desc_1.4.2       \n[17] lifecycle_1.0.4   compiler_4.3.1    stringr_1.5.1     brio_1.1.3       \n[21] httpuv_1.6.12     htmltools_0.5.6.1 usethis_2.2.2     yaml_2.3.8       \n[25] pkgdown_2.0.7     tidyr_1.3.0       later_1.3.1       pillar_1.9.0     \n[29] crayon_1.5.2      urlchecker_1.0.1  ellipsis_0.3.2    cranlogs_2.1.1   \n[33] rsconnect_1.1.1   cachem_1.0.8      sessioninfo_1.2.2 mime_0.12        \n[37] tidyselect_1.2.0  digest_0.6.33     stringi_1.8.3     dplyr_1.1.4      \n[41] purrr_1.0.2       rprojroot_2.0.3   fastmap_1.1.1     cli_3.6.2        \n[45] magrittr_2.0.3    pkgbuild_1.4.2    utf8_1.2.4        withr_3.0.0      \n[49] waldo_0.5.1       prettyunits_1.2.0 promises_1.2.1    rappdirs_0.3.3   \n[53] roxygen2_7.3.0    rmarkdown_2.25    httr_1.4.7        gitcreds_0.1.2   \n[57] stats_4.3.1       memoise_2.0.1     shiny_1.8.0       evaluate_0.22    \n[61] knitr_1.45        miniUI_0.1.1.1    profvis_0.3.8     rlang_1.1.3      \n[65] Rcpp_1.0.11       xtable_1.8-4      glue_1.7.0        xml2_1.3.5       \n[69] pkgload_1.3.3     rstudioapi_0.15.0 jsonlite_1.8.7    R6_2.5.1         \n[73] fs_1.6.3         \n\n\n### Where is the test code located for these tests?\nplease enter manually\n\n### Where the test code is located in a git repository, add the git commit SHA\nplease enter manually, if relevant",
    "crumbs": [
      "Home",
      "Documentation",
      "Writing function tests"
    ]
  },
  {
    "objectID": "testing.html#hints-for-working-with-github",
    "href": "testing.html#hints-for-working-with-github",
    "title": "Writing function tests",
    "section": "Hints for working with GitHub",
    "text": "Hints for working with GitHub\nRStudio has a built-in git interface, which is a good way to manage your git repositories if you use RStudio.\nThe Happy Git and GitHub for the useR book is a comprehensive guide to working with git and GitHub. Of particular use are chapters 9 to 12 on connecting your computer with GitHub.\nThe GitHub desktop app is a good way to manage your git repositories if you are not comfortable with the command line. This is also an easy way to connect your computer with your GitHub account. There are many other GUIs for working with git repositories. See here for a listing of some of them.",
    "crumbs": [
      "Home",
      "Documentation",
      "Writing function tests"
    ]
  },
  {
    "objectID": "risk_metrics.html",
    "href": "risk_metrics.html",
    "title": "Risk metrics",
    "section": "",
    "text": "This document describes the metrics used by members of the Swiss Clinical Trial Organization when evaluating add on packages of the statistical environment, R. It provides a table of the metrics and a short explanation of the considerations behind each of the metrics.\nAssessing the risk associated with using a software package is one of the steps required for working under a validated environment, as determined in the SCTO R-validation policy. The risk of (using) a package is the opposite of the confidence we have in the package’s delivered output: packages which we are confident in using are of low risk, and vice versa. The metrics described below are the building stones in determining this risk. A final risk score is determined based on a weighting scheme including these metrics.\nFollowing this assessment, the assessor determines whether any parts of the package need to be tested (function/unit testing) for the package’s intended use in a specific product. The documentation of any actions following the initial risk assessment is listed in a separate table.\nThis document is part of the SCTO’s R validation policy (NAME – LOCATION – VERSION) and refers to the standard operation procedure for Package Testing (NAME – LOCATION) of the SCTO statistics platform.\nThe metrics are written with the aim of assessing R packages; small adaptations may be required for packages of other programs such as Stata or SAS.\nBased on the principals below, the table in the addendum provides the metrics’ values to be documented upon package evaluation for the SCTO platform statistics. Note that the evaluation is per package with a specific version number. Package version changes and updates require re-assessment of the metrics for the new version.",
    "crumbs": [
      "Home",
      "Documentation",
      "Risk metrics"
    ]
  },
  {
    "objectID": "risk_metrics.html#objective",
    "href": "risk_metrics.html#objective",
    "title": "Risk metrics",
    "section": "",
    "text": "This document describes the metrics used by members of the Swiss Clinical Trial Organization when evaluating add on packages of the statistical environment, R. It provides a table of the metrics and a short explanation of the considerations behind each of the metrics.\nAssessing the risk associated with using a software package is one of the steps required for working under a validated environment, as determined in the SCTO R-validation policy. The risk of (using) a package is the opposite of the confidence we have in the package’s delivered output: packages which we are confident in using are of low risk, and vice versa. The metrics described below are the building stones in determining this risk. A final risk score is determined based on a weighting scheme including these metrics.\nFollowing this assessment, the assessor determines whether any parts of the package need to be tested (function/unit testing) for the package’s intended use in a specific product. The documentation of any actions following the initial risk assessment is listed in a separate table.\nThis document is part of the SCTO’s R validation policy (NAME – LOCATION – VERSION) and refers to the standard operation procedure for Package Testing (NAME – LOCATION) of the SCTO statistics platform.\nThe metrics are written with the aim of assessing R packages; small adaptations may be required for packages of other programs such as Stata or SAS.\nBased on the principals below, the table in the addendum provides the metrics’ values to be documented upon package evaluation for the SCTO platform statistics. Note that the evaluation is per package with a specific version number. Package version changes and updates require re-assessment of the metrics for the new version.",
    "crumbs": [
      "Home",
      "Documentation",
      "Risk metrics"
    ]
  },
  {
    "objectID": "risk_metrics.html#scope",
    "href": "risk_metrics.html#scope",
    "title": "Risk metrics",
    "section": "Scope",
    "text": "Scope\nThis document is applicable to all R add-on packages. Out of scope are packages included in the official R-distribution including “Base R” packages. Also out of scope is the collection of “recommended packages”, developed and validated by members of the R Development Core Team, as listed in the document “R: Regulatory Compliance and Validation Issues, The R Foundation for Statistical Computing, 2021”. R-base and recommended packages are approved per definition, as stated in the policy document.",
    "crumbs": [
      "Home",
      "Documentation",
      "Risk metrics"
    ]
  },
  {
    "objectID": "risk_metrics.html#metric-explanation---glossary",
    "href": "risk_metrics.html#metric-explanation---glossary",
    "title": "Risk metrics",
    "section": "Metric explanation - glossary",
    "text": "Metric explanation - glossary\nThe following metrics are considered when assessing a package. These are listed in Appendix-1 below. Here we provide a precise definition:\n\nPackage name, version, release date\nversion and release date of the specific version assessed at this moment. These are required for identifying the package and for documentation. Packages will be re-assessed upon update and version changes; documentation of older versions remains in the document. These values are not included in the risk calculation.\n\n\nPurpose (statistical_package):\nWe define three risk levels for a package, depending on the package’s purpose and methodology:\n\n“non-statistical” packages: packages that deal only with data-wrangling and manipulation (e.g., dplyr) or with reporting processes (e.g., Sweave, xtable). Such processes are of ‘low risk’ as no statistical calculations are performed, and data-errors are, comparatively, easy to detect. Similarly, packages associated with application interfaces such as Shiny application are considered “non-statistical”.\n“Statistical with published methods”: packages that perform statistical calculations based on known methods, or on methods that have been published in peer reviewed journals. These packages obtain a “medium risk” status.\n“Statistical non-published methods”: packages that perform statistical calculations, but the underlying methods have not been published in a peer reviewed journal. These packages obtain a status “high risk”.\n\n\n\nAuthor (author):\nThe author(s) of a package will be viewed as indicator for its trustworthiness. If package authors (noted as ‘aut’ in the package description, e.g., as listed on CRAN) are well-known within the statistical, data-science and R communities and have credentials based on their qualifications, education, present and past affiliations, the risk of the respective package will be scored as low. If package authors have credentials based on their qualifications, education, present and past affiliations, but are not well-known within the statistical, data-science and R communities, the risk of the respective package will be scored as medium.  If package authors are not well-known within the statistical, data-science and R communities and have no clear credentials based on their qualifications, education, present and past affiliations, the risk of the respective package will be scored as high.\nNote that whether an author is ‘well-known’ in the community is a subjective assessment and accepted as such; in addition, groups of authors are evaluated as a collective.\n\n\nMaintainer (maintainer) :\nThe package has a named maintainer who’s contact details (email) are available and published. A positive answer provides a “low risk” score. A package with no named maintainer is scored as high. The rationale behind this metric lies in the fact that the indication of name and email is evidence of a package’s active maintenance and availability of contact in case of bugs and/or suggestions.\n\n\nNumber of dependencies (nr_dependencies):\nThis metric assesses the level of risk associated with the number of dependencies a package relies on. Dependencies are other packages or processes that the evaluated package depends on, as listed in CRAN under “depends” and/or “imports”. The risk of unexpected behaviour increases with the number of dependencies, since there is a greater likelihood of issues on a specific routine if updates are performed on a dependent package. Great care should be considered while using packages involving many dependencies.\nThe listed number is converted into a [0, 1] score, with 0 representing low number of dependencies (= low risk) and 1 representing many dependencies (= high risk). Taking a similar approach for the transforming the number to a score as the ‘riskmetric’ package (R validation hub, 2023), we use a simplification of the classic logistic curve as a scoring function. A sigmoid midpoint is 4 reverse dependencies, ie., x[0] = 4 and logistic growth rate of k = 0.5.\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n\n\n\n\n\n\n\nETC.",
    "crumbs": [
      "Home",
      "Documentation",
      "Risk metrics"
    ]
  },
  {
    "objectID": "pkg_validation.html",
    "href": "pkg_validation.html",
    "title": "Risk assessing packages",
    "section": "",
    "text": "This page functions as the standard operating procedure for the risk assessment of R packages within the SCTO Statistics & Methodology Platform framework.\nIn order to perform a risk assessment:\n\nfirst, check whether the package has already been risk assessed. This can be done by any of the following methods:\n\nthe check_session function from the R package validation can be used from within your R session to check if a package has been risk assessed.\ncheck the table on the home page of this site\ncheck for the package in the package validation GitHub repository by searching for it among the issues using the search bar towards the top of the page (remember to remove the is:open filter). E.g. to search for dplyr, the search might be is:issue dplyr\n\nif it has been risk assessed, check the associated risk and consider whether any functions within package need additional testing for your use case.\nif it has not been risk assessed, go to the package validation GitHub repository\n\nclick the green “New issue” button (towards the top right)\nselect the New package validation template by clicking the green Get started button on the right\nfill in the form, following the instructions provided. Additional notes on the risk metrics can be found on the Risk metrics page.\n\nNote that the R package validation contains various useful functions to help with the validation process. Especially useful are:\n\nget_n_deps, which returns the number of dependencies of a package,\nget_12month_downloads which returns the number of downloads of a package in the last 12 months, and\nget_release_date finds the release date of a package.\n\n\nOnce you have completed the form, click the green “Submit new issue” button at the bottom of the page.\nSubmitting the form will trigger an automated job that will calculate the risk based on the information you entered into the form and post a comment on the issue.\n\n\n\n\n\n\n\nflowchart TD\n  A[Identify the package that may need risk assessment] --&gt; B(Check whether the package has already been risk assessed)\n  B --&gt; C1(\"`Within your R session, load the package and use the function \n  &lt;span style=\"font-family:Courier; font-size: 80%\"&gt;validation::check_session()&lt;/span&gt;`\")\n  B --&gt; C2(Check whether the package is listed on the &lt;a href='index.html'&gt;home page&lt;/a&gt;)\n  B --&gt; C3(Check for an issue on the &lt;a href='https://github.com/SwissClinicalTrialOrganisation/pkg_validation/issues'&gt;GitHub&lt;/a&gt;)\n  C1 --&gt; D(RA exists?)\n  C2 --&gt; D\n  C3 --&gt; D\n  D --Yes--&gt; E[The associated risk level of the package]\n  D --No--&gt; F(Perform a risk assessment following the instructions on &lt;a href='https://github.com/SwissClinicalTrialOrganisation/pkg_validation/issues/new/choose'&gt;GitHub&lt;/a&gt;)\n  F --&gt; G(Open a new issue with the 'New package validation' template)\n  G --&gt; H(Fill out the form following the instructions provided)\n  H --&gt; J(Submit the issue and wait for the automated risk assessment)\n  J --&gt; E\n  E --&gt; K(Considering your use case, do you need to test any functions within the package?)\n  K --No--&gt; N(Use the package)\n  K --Yes--&gt; O(Check the validation_tests repository to see if any functions from the package have been tested)\n  O --&gt; P(Tests are available, and they cover your use case)\n  O --&gt; Q(Tests are available, but they do not cover your use case)\n  O --&gt; R(No tests are available)\n  P --&gt; N\n  Q --&gt; S(Write new tests for the functions you need and submit them to the validation_tests repository)\n  R --&gt; S\n  S --&gt; T(Ensure tests pass)\n  T --&gt; N",
    "crumbs": [
      "Home",
      "Documentation",
      "Risk assessing packages"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This site provides access to the R package validation platform from statisticians of the SCTO Statistics & Methodology Platform.\nStatisticians of the SCTO take a risk based approach to package utilisation, whereby the risk of the product (e.g. report) defines how validated a package must be to be used in the product.\nR – package risk (baseline risk)\n      \n    \n    \n      Low\n      Medium\n      High\n    \n  \n  \n    Product\nrisk\nLow\n\nAllowed\n\nAllowed\n\nAllowed\n\n    Medium\n\nAllowed\n\nAllowed\n\nNeeds assessment\n\n    High\n\nAllowed\n\nNeeds assessment\n\nNeeds assessment",
    "crumbs": []
  },
  {
    "objectID": "index.html#packages-with-risk-assessments",
    "href": "index.html#packages-with-risk-assessments",
    "title": "Welcome",
    "section": "Packages with risk assessments",
    "text": "Packages with risk assessments\n19 packages or package versions have been risk assessed within the SCTO Statistics & Methodology Platform so far.\n\n\n\n\n\n\n\n\n\nThe following packages have newer versions available, which may be an indication that the risk assessment is outdated:\n\n\n# A tibble: 0 × 3\n# ℹ 3 variables: Package &lt;chr&gt;, Validated version &lt;chr&gt;,\n#   Available version &lt;chr&gt;",
    "crumbs": []
  },
  {
    "objectID": "index.html#function-tests",
    "href": "index.html#function-tests",
    "title": "Welcome",
    "section": "Function tests",
    "text": "Function tests\n12 packages or package versions have been tested by the SCTO Statistics & Methodology Platform so far.",
    "crumbs": []
  },
  {
    "objectID": "policy.html",
    "href": "policy.html",
    "title": "SCTO Validation Policy",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\n\n\n\n\nName\nDate\n\n\n\n\nAuthored/Revised by\n\n\n\n\nReviewed by\n\n\n\n\nReleased by\n\n\n\n\n\nVersion History\n\n\n\nVersion\nDate\nAuthor\nSummary of Changes\n\n\n\n\n0.1\n2024-04-18\nAlan Haynes\nInitial draft",
    "crumbs": [
      "Home",
      "Documentation",
      "SCTO Validation Policy"
    ]
  },
  {
    "objectID": "policy.html#scope-and-background",
    "href": "policy.html#scope-and-background",
    "title": "SCTO Validation Policy",
    "section": "Scope and background",
    "text": "Scope and background\nThis document describes the policy recommended by the Swiss Clinical Trial Organization (SCTO) for the use of R in a validated environment for purposes of clinical research. R is a ‘language and environment for statistical computing and graphics’ (source: the R file). R is an official part of the Free Software Foundation’s GNU project and is released under the Free Software Foundation’s GNU Public License.\nGoal is in general to describe the policy, and specifically to define concepts for R. Some principles can be shared also for other software.\nIn scope:\n\nR specific processes\nProcesses shared by the SCTO platform. A baseline level on which individual organizations (CTUs) can go more strict\n\nOut of scope:\n\nValidation of ‘off the shelf’ software (?)\nInternal, organization specific, processes. Necessary processes will be mentioned, but not detailed/defined.",
    "crumbs": [
      "Home",
      "Documentation",
      "SCTO Validation Policy"
    ]
  },
  {
    "objectID": "policy.html#definitions",
    "href": "policy.html#definitions",
    "title": "SCTO Validation Policy",
    "section": "Definitions",
    "text": "Definitions\n\nDefine validation and verification validation (extract ICH E9 expert working group)\nShort description of “The R environment” including what are the base and recommended packages.\n\nETC",
    "crumbs": [
      "Home",
      "Documentation",
      "SCTO Validation Policy"
    ]
  },
  {
    "objectID": "running_tests.html",
    "href": "running_tests.html",
    "title": "Running function tests",
    "section": "",
    "text": "Although the unit of testing is the function, for organisational purposes, the tests are grouped by package. This is because the tests are run in the context of the package, and eases the process of running and reporting tests.\nTests are run via the test function in the validation package:\n\nvalidation::test(\"accrualPlot\")\n\nThe function will download the testing files from GitHub, run the tests and format the results for easier copy/pasting into a reporting issue on GitHub.\n\n\nWarning: package 'testthat' was built under R version 4.3.3\n\n\nLoading required package: accrualPlot\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'accrualPlot'\n\n\nInstalling package into 'D:/a/_temp/Library'\n(as 'lib' is unspecified)\n\n\npackage 'accrualPlot' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\runneradmin\\AppData\\Local\\Temp\\RtmpuELsfB\\downloaded_packages\n\n\nWarning: package 'accrualPlot' was built under R version 4.3.3\n\n\nLoading required package: lubridate\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n✔ | F W  S  OK | Context\n\n⠏ |          0 | accrual_create_df                                              \n✔ |          8 | accrual_create_df\n\n⠏ |          0 | summary                                                        \n✔ |         10 | summary\n\n══ Results ═════════════════════════════════════════════════════════════════════\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 18 ]\n\n\nWarning in rm(accrualdemo): object 'accrualdemo' not found\n\n\nError in gh(endpoint = \"/user\", .token = .token, .api_url = .api_url,  : \n  GitHub API error (403): Resource not accessible by integration\nℹ Read more at\n  &lt;https://docs.github.com/rest/users/users#get-the-authenticated-user&gt;\nError in Sys.info()$user : $ operator is invalid for atomic vectors\n\n\n## Copy and paste the following output into the indicated sections of a new issue\n\nISSUE NAME: \n[Package test]: accrualPlot version 1.0.7 \n\n### Name \nUnknown (probably GitHub Action bot) \n\n### Name of the package you have validated \naccrualPlot \n\n### What version of the package have you validated? \n1.0.7 \n\n### Where was the package from? \nCRAN (R 4.3.3) \n\n### Package repository version reference\nNA \n\n### When was this package tested? \n2024-07-31 \n\n### What was tested? \nTests for package accrualPlot \n - `summary` produces expected results \n - `accrual_create_df` produces expected results \n  \n These tests are primarily for testing the validation infrastructure. accrualPlot \n has extensive tests \n \n\n### Test results \nPASS \n\n### Test output:\n\n\n|file                     |context           |test                     | nb| passed|skipped |error | warning|\n|:------------------------|:-----------------|:------------------------|--:|------:|:-------|:-----|-------:|\n|test-accrual_create_df.R |accrual_create_df |monocentric              |  4|      4|FALSE   |FALSE |       0|\n|test-accrual_create_df.R |accrual_create_df |multicentric             |  4|      4|FALSE   |FALSE |       0|\n|test-summary.R           |summary           |monocentric as expected  |  5|      5|FALSE   |FALSE |       0|\n|test-summary.R           |summary           |multicentric as expected |  5|      5|FALSE   |FALSE |       0|\n\n### SessionInfo:\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] lubridate_1.9.3  testthat_3.2.1.1\n\nloaded via a namespace (and not attached):\n [1] rappdirs_0.3.3    utf8_1.2.4        generics_0.1.3    tidyr_1.3.1      \n [5] stringi_1.8.4     hms_1.1.3         digest_0.6.36     magrittr_2.0.3   \n [9] grid_4.3.0        evaluate_0.24.0   timechange_0.3.0  pkgload_1.4.0    \n[13] fastmap_1.2.0     rprojroot_2.0.4   jsonlite_1.8.8    sessioninfo_1.2.2\n[17] cranlogs_2.1.1    brio_1.1.5        httr_1.4.7        validation_0.3.6 \n[21] purrr_1.0.2       fansi_1.0.6       scales_1.3.0      httr2_1.0.2      \n[25] cli_3.6.3         rlang_1.1.4       crayon_1.5.3      gitcreds_0.1.2   \n[29] munsell_0.5.1     withr_3.0.0       yaml_2.3.10       tools_4.3.0      \n[33] accrualPlot_1.0.7 tzdb_0.4.0        dplyr_1.1.4       colorspace_2.1-1 \n[37] ggplot2_3.5.1     curl_5.2.1        vctrs_0.6.5       R6_2.5.1         \n[41] lifecycle_1.0.4   stringr_1.5.1     htmlwidgets_1.6.4 waldo_0.5.2      \n[45] pkgconfig_2.0.3   desc_1.4.3        gtable_0.3.5      pillar_1.9.0     \n[49] glue_1.7.0        gh_1.4.1          xfun_0.46         tibble_3.2.1     \n[53] tidyselect_1.2.1  knitr_1.48        htmltools_0.5.8.1 rmarkdown_2.27   \n[57] readr_2.1.5       pkgsearch_3.1.3   compiler_4.3.0   \n\n\n### Where is the test code located for these tests?\nSwissClinicalTrialOrganisation/validation_tests\n\n### Where the test code is located in a git repository, add the git commit SHA\na696e6b665e9e8ea761c00d4126e86cc3defc816\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt may be that the package being tested, or indeed functions in the testing infrastructure (testthat and waldo) have been updated since the tests were authored. This may lead to tests failing that previously passed. In this case, the tests should be updated to reflect the new behaviour.\nOne particular case is a change in attributes. For example, the coef method for a model object returns a named vector, but we might compare the individual coeffients to a vector of expected values. There is thus a potential mismatch in the attributes of the two vectors, which may cause the test to fail with a names for target but not for current message. Use of the third edition of testthat (v3.0.0) should help to avoid this issue.",
    "crumbs": [
      "Home",
      "Documentation",
      "Running function tests"
    ]
  }
]