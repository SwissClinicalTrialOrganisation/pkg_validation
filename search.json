[
  {
    "objectID": "wi/testing.html",
    "href": "wi/testing.html",
    "title": "Writing function tests",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\n\n\n\n\nName\nDate\n\n\n\n\nAuthored/Revised by\n\n\n\n\nReviewed by\n\n\n\n\nReleased by\n\n\n\n\n\nVersion History\n\n\n\n\n\n\n\n\n\nVersion\nDate\nAuthor\nSummary of Changes\n\n\n\n\n0.1\n2024-08-24\nAlan Haynes, Elio Carreras, Lisa Hofer, Michael Coslovsky, Christina\n\n\n\nHuf, Christine Otieno\nInitial draft"
  },
  {
    "objectID": "wi/testing.html#working-instructions",
    "href": "wi/testing.html#working-instructions",
    "title": "Writing function tests",
    "section": "Working instructions",
    "text": "Working instructions\nThe working instructions for the testing process are found under the relevant SCTO GitHub page (https://swissclinicaltrialorganisation.github.io/validation/articles/contribute.html). A copy of the vignette is an associated document to this SOP. Below we provide only a bullet-point description of the relevant steps:\n\nFunction tests are performed and stored on the SCTO’s statistics platform’s validation_tests repository\nThe repository contains both the test script and the data frames used for the tests when relevant\nApproving the test is conditional on a review by a second SCTO statistician from a different CTU than the test author. Reviews are based on the “four eyes” principle.\nThe SCTO function-tests platform collects relevant meta-data for the tests including:\n\nTest author\nTest timing (time of submitting the test to the repository)\nWho reviewed the test code\nWhen was the review performed\nWhich function was tested: function name within package and package version\nWhich output of the function was tested\nWhat type of testing was performed\nTest result (pass/fail)\nEvidence of test (copy of console output)\nSession information of the testing environment\n\nThe SCTO develops a package that allows easy rerun of package tests upon change of R or package version, as well as generating a report for testing.\n\nFunction tests are stored in the validation_tests repository at https://github.com/SwissClinicalTrialOrganisation/validation_tests. Tools to create the testing structure for each tested package and run the tests are provided in the validation R package.\n\nSubmitting the test for incorporation into the framework\nInstructions for writing tests and incorporating them into the validation_tests repository are provided here. Prior to incorporation, the test(s) will be reviewed according to the criteria below.\n\n\nRunning tests\nThe output from test can then be used to complete the function test issue form on GitHub. All functions from a particular package can be reported together in a single report.\nHere, we distinguish between tests that are already implemented within a package, and those that have been developed as part of the SCTO framework."
  },
  {
    "objectID": "wi/testing.html#approving-the-test",
    "href": "wi/testing.html#approving-the-test",
    "title": "Writing function tests",
    "section": "Approving the test",
    "text": "Approving the test\nA consistency review is performed based on the “four eyes” principle where the reviewer should be chosen from a different participating institution than the author of the test. The review should address the following points:\n\nIs the listed meta-data complete?\nIs the level of testing specified appropriately?\nDoes the test follow the procedure described in this guideline?\n\nThis review is done in effect by approving the ‘pull request’ on GitHub.\nThe reviewer comments the function test. Only after approval by the reviewer, the function will be listed as tested on the SCTO platform."
  },
  {
    "objectID": "wi/testing.html#documentation",
    "href": "wi/testing.html#documentation",
    "title": "Writing function tests",
    "section": "Documentation:",
    "text": "Documentation:\nTest reports are made on the pkg_validation repository at https://github.com/SwissClinicalTrialOrganisation/pkg_validation/issues . Select “New issue”. Select the “Add package/function testing results”. Fill out the predefined form, using the output from R (validation::test(“packagename”)).\n\n\n\n\n\n\n\nWhat to report\nDetails\n\n\n\n\nWho\nWho performed the test?\n\n\nWhen\nWhen was the test performed (date)\n\n\nWhat\nWhich function from which package and package version. If the function does not come from a package (e.g. it’s a script that is stored as a GitHub Gist), the link to the code should be provided.\n\n\nTest details\nWhat precisely was tested? This could be a link to the tests or a reference to the package and function containing the tests together with that packages version number.\n\n\nDegree Type of testing\nWhich pathway in the decision tree was followed:\n\n“re-run prior test”\n\n\n\nReview existing package test code\n\n\n\nDeterministic process\n\n\n\nComparison to other implementation\n\n\n\nSimulation Was the testing comprehensive, superficial, or something in between?\n\n\n\nTest result\nPass/fail\n\n\nEvidence of test\nCopy/paste of the console output.\n\n\nSessionInfo\nRelevant parts of sessionInfo:\n\nR version\nOS\nWhich other packages and versions were loaded?\n\n\n\n\nFailed tests should also be reported. When a test fails, a bug report should be posted via the appropriate route for that package (e.g. a github issue to the package repository, an email to a specific address). The bug report should also be noted alongside the test results, where possible providing a link to the report (e.g. to the GitHub issue), and followed up on by the individual discovering the bug. When the bug has been fixed, the tests can be run again and the successful test result recorded as above."
  },
  {
    "objectID": "wi/review_tests.html",
    "href": "wi/review_tests.html",
    "title": "Reviewing tests",
    "section": "",
    "text": "Once someone has prepared one or more tests for functions in a package and submitted them to be incorporated into the platform via a pull request. They should then be reviewed by another member of the platform, ideally from another unit, to check that the tests are programmed and documented appropriately.\nflowchart LR\n  a[Tests written] --&gt; b[Pull request to incorporate]\n  b --&gt; c[Reviewers chosen] \n  c --&gt; d[Code reviewed]\n  d --&gt; e[Review approved]\n  d --&gt; f[Suggested improvements]\n  f --&gt; g[Author improves tests]\n  g --&gt; d",
    "crumbs": [
      "Work Intructions",
      "Reviewing tests"
    ]
  },
  {
    "objectID": "wi/review_tests.html#who-reviews",
    "href": "wi/review_tests.html#who-reviews",
    "title": "Reviewing tests",
    "section": "Who reviews?",
    "text": "Who reviews?\nAll pull requests are by default automatically assigned to one individual per unit. These individuals should agree among themselves who can perform the review, potentially nominating someone else from their unit. Those that will not be performing the review can be removed from the list of assignees.\n\n\n\n\n\n\nNote\n\n\n\nIf you need a review urgently, reach out to someone by other means (e.g. email) and arrange that they perform the review for you.",
    "crumbs": [
      "Work Intructions",
      "Reviewing tests"
    ]
  },
  {
    "objectID": "wi/review_tests.html#performing-the-review",
    "href": "wi/review_tests.html#performing-the-review",
    "title": "Reviewing tests",
    "section": "Performing the review",
    "text": "Performing the review\nThe review is performed within the pull request on GitHub. There are four main tabs the pull request screen on GitHub:\n\n\n\n\n\n\nThe conversation tab is for discussions of general points about the pull request.\nThe commits tab lists the individual commits that make up the pull request. For our purposes, this is rarely of use.\nThe checks tab shows the results of automated checks that are run on the pull request. As we have no automated checks running for this repository, this tab is also not useful.\nThe files changed tab shows the changes that have been made in the pull request. This is where the review is performed.\n\nThe files changes lists all changes in all files modified during the pull request. The reviewer should look at each file in turn, checking that the changes are appropriate and that the code is well written and documented. The reviewer should also check that the tests are appropriate and that they test the correct things. Also, ensure that the details of the test in info.txt match the tests that were actually performed.\nWhere there are general points to be made, these should be made in the conversation tab. For specific points, the reviewer can comment on the specific line of code in the files tab.\n\nhover over the line to be commented on\nclick on the + that appears\ntype the comment in the box that appears\n\n\n\n\n\n\n\nclick on Start a review to submit the comment\n\nTo indicate that you have checked a particular file and that it is suitable, you can click on the Viewed box on the top right of each file.\nThe very top of the page has a box for finalising the review.\n\n\n\n\n\nIf there are no issues, the review can be marked with approve. If you have suggestions or require modifications, you can mark it as request changes. If you have questions, you can mark it as comment or request changes, whichever is most appropriate, and enter your questions in the box.",
    "crumbs": [
      "Work Intructions",
      "Reviewing tests"
    ]
  },
  {
    "objectID": "wi/review_tests.html#incorporating-the-tests-in-the-repository",
    "href": "wi/review_tests.html#incorporating-the-tests-in-the-repository",
    "title": "Reviewing tests",
    "section": "Incorporating the tests in the repository",
    "text": "Incorporating the tests in the repository\nOnce the tests have been reviewed and found to be suitable and appropriately documented, the pull request needs to be merged into the repository. Each CTU has at least one nominated individual that can perform a merge (typically the same individuals distributing reviews). This individual should check that the review has been approved and then merge the pull request using the green button at the bottom of the conversation tab, followed by the “confirm merge” button that appears afterwards.",
    "crumbs": [
      "Work Intructions",
      "Reviewing tests"
    ]
  },
  {
    "objectID": "sop/sop_fn_tests.html",
    "href": "sop/sop_fn_tests.html",
    "title": "Function testing SOP",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\n\n\n\n\nName\nDate\n\n\n\n\nAuthored/Revised by\n\n\n\n\nReviewed by\n\n\n\n\nReleased by\n\n\n\n\n\nVersion History\n\n\n\n\n\n\n\n\n\nVersion\nDate\nAuthor\nSummary of Changes\n\n\n\n\n0.1\n2024-07-31\nAlan Haynes, Elio Carreras, Lisa Hofer, Michael Coslovsky, Christina Huf, Christine Otieno\nInitial draft",
    "crumbs": [
      "SOPs",
      "Function testing SOP"
    ]
  },
  {
    "objectID": "sop/sop_fn_tests.html#purpose",
    "href": "sop/sop_fn_tests.html#purpose",
    "title": "Function testing SOP",
    "section": "Purpose",
    "text": "Purpose\nThis SOP defines the process and steps that need to be followed when testing the performance of specific program functions within the SCTO-Statistics’ platform computer system validation. The process is written to serve as a standardized approach for function testing. A standardized process will allow different organizations to rely on results of tests performed also outside of the internal unit, improving efficiency overall. The centralized process can then be implemented in all associated units as the minimal procedure. Organizations may go beyond the process described herein, as long as they do not contradict the central process.\nIn lay words, the process should allow a member (with focus on statisticians and data-scientists) of any participating institution (SCTO associated organization, including SAKK, the biostatistics’ department of the university of Zurich etc.) to understand and reproduce function tests saved and documented on the designated SCTO platform infrastructure. In addition, the process ensures that updates of packages or functions can rerun the tests in a fast and straightforward manner, so that package updates can be validated easily.",
    "crumbs": [
      "SOPs",
      "Function testing SOP"
    ]
  },
  {
    "objectID": "sop/sop_fn_tests.html#abbreviations-and-definitions",
    "href": "sop/sop_fn_tests.html#abbreviations-and-definitions",
    "title": "Function testing SOP",
    "section": "Abbreviations and definitions",
    "text": "Abbreviations and definitions\n\nSOP\n\nStandard operating procedure\n\nR\n\nThe R computer program\n\nUnit testing\n\nThe testing one one particular element of a program\n\nFunction\n\nA specific program within the R language\n\nTest author\n\nAny employee of an SCTO associated organization writing a function test according to this SOP\n\nTester\n\nAny employee of an SCTO associated organization executing and documenting a function test according to this SOP\n\nReviewer\n\nAny employee of an SCTO associated organization reviewing a function test from a different test author according to this SOP",
    "crumbs": [
      "SOPs",
      "Function testing SOP"
    ]
  },
  {
    "objectID": "sop/sop_fn_tests.html#scope",
    "href": "sop/sop_fn_tests.html#scope",
    "title": "Function testing SOP",
    "section": "Scope",
    "text": "Scope\nThis SOP defines the process for:\n\nUnit testing\nSelf-written functions stored on the SCTO platform infrastructure.\nSpecific functions from software used in the institutes that have been determined as requiring testing.\n\nIn scope are functions of the R computing environment, written within or out of the organization, for which the necessity of unit functional testing was determined by a member of a participating institute according to the SCTO R-validation policy. The process may be adapted for other statistical packages. The SOP focuses primarily on functions used for statistical reporting and data-wrangling.\nThe appropriate level of testing required is determined within the SOP, but the decision to test the function is out of scope. According to the SCTO R validation policy, this decision is determined from the risk associated with the intended use of the function within a specific product or project, future use and the software packages being used for the product. The SCTO statistics’ platform’s high level risk assessment as well as the SCTO policy for the use of R in a validated environment describe the processes to follow in assessing this risk and in determining whether specific function testing should be performed.\nETC.",
    "crumbs": [
      "SOPs",
      "Function testing SOP"
    ]
  },
  {
    "objectID": "sop/metrics.html",
    "href": "sop/metrics.html",
    "title": "SCTO R-Package Risk Metrics",
    "section": "",
    "text": "Authors\n\n\n\n\n\n\n\n\n\n\nName\nPosition\nOrg anization\nAddress\nemail\n\n\n\n\nMichael Coslovsky, PhD\nHead da ta-analysis\nD epartment Clinical Research, U niversity of Basel\nSpit alstrasse 12, 4031, Basel\nMichael.c oslovsky@usb.ch\n\n\nNicole Graf\nS tatistician\nClinical Trials Unit, Kant onsspital St. Gallen\nHaus 89,\nRorschach erstrasse 95, 9007 St. Gallen\nNico le.graf@kssg.ch\n\n\nJulien Sauser\nS tatistician\nCHUV\n\nJulien .sauser@chuv.ch\n\n\nChristina Huf\nHead Quality Management\nD epartment Clinical Research, U niversity of Bern\n\nchristi na.huf@unibe.ch\n\n\nElio Carreras\nSenior Statistical Programmer\nSAKK\n\nElio.C arreras@sakk.ch\n\n\n\nDate\n2024-XX-XX\n\nObjective\nThis document describes the metrics used by members of the Swiss Clinical Trial Organization when evaluating add on packages of the statistical environment, R. It provides a table of the metrics and a short explanation of the considerations behind each of the metrics.\nAssessing the risk associated with using a software package is one of the steps required for working under a validated environment, as determined in the SCTO R-validation policy. The risk of (using) a package is the opposite of the confidence we have in the package’s delivered output: packages which we are confident in using are of low risk, and vice versa. The metrics described below are the building stones in determining this risk. A final risk score is determined based on a weighting scheme including these metrics.\nFollowing this assessment, the assessor determines whether any parts of the package need to be tested (function/unit testing) for the package’s intended use in a specific product. The documentation of any actions following the initial risk assessment is listed in a separate table.\nThis document is part of the SCTO Computerized Systems Validation Policy for R (NAME – LOCATION – VERSION) and refers to the standard operation procedure for Function testing (NAME – LOCATION) of the SCTO statistics platform.\nThe metrics are written with the aim of assessing R packages.\nBased on the principals below, the table in the addendum provides the metrics’ values to be documented upon package evaluation for the SCTO platform statistics. Note that the evaluation is per package with a specific version number. Package version changes and updates require re-assessment of the metrics for the new version.\n\n\nSCOPE\nThis document is applicable to all R add-on packages. Out of scope are packages included in the official R-distribution including “Base R” packages. Also out of scope is the collection of “recommended packages”, developed and validated by members of the R Development Core Team, as listed in the document “R: Regulatory Compliance and Validation Issues, The R Foundation for Statistical Computing, 2021”. R-base and recommended packages are approved per definition, as stated in the policy document.\n\n\nMETRIC EXPLANATION - glossary\nThe following metrics are considered when assessing a package. These are listed in Appendix-1 below. Here we provide a precise definition:\n\nPackage name, version, release date: version and release date of the specific version assessed at this moment. These are required for identifying the package and for documentation. Packages will be re-assessed upon update and version changes; documentation of older versions remains in the document. These values are not included in the risk calculation.\nPurpose (statistical_package) We define three risk levels for a package, depending on the package’s purpose and methodology:\n\n“non-statistical” packages: packages that deal only with data-wrangling and manipulation (e.g., dplyr) or with reporting processes (e.g., Sweave, xtable). Such processes are of ‘low risk’ as no statistical calculations are performed, and data-errors are, comparatively, easy to detect. Similarly, packages associated with application interfaces such as Shiny application are considered “non-statistical”.\n“Statistical with published methods”: packages that perform statistical calculations based on known methods, or on methods that have been published in peer reviewed journals. These packages obtain a “medium risk” status.\n“Statistical non-published methods”: packages that perform statistical calculations, but the underlying methods have not been published in a peer reviewed journal. These packages obtain a status “high risk”.\n\nAuthor (author): The author(s) of a package will be viewed as indicator for its trustworthiness. If package authors (noted as ‘aut’ in the package description, e.g., as listed on CRAN) are well-known within the statistical, data-science and R communities and have credentials based on their qualifications, education, present and past affiliations, the risk of the respective package will be scored as low. If package authors have credentials based on their qualifications, education, present and past affiliations, but are not well-known within the statistical, data-science and R communities, the risk of the respective package will be scored as medium. If package authors are not well-known within the statistical, data-science and R communities and have no clear credentials based on their qualifications, education, present and past affiliations, the risk of the respective package will be scored as high.\nNote that whether an author is ‘well-known’ in the community is a subjective assessment and accepted as such; in addition, groups of authors are evaluated as a collective.\nMaintainer (maintainer): The package has a named maintainer who’s contact details (email) are available and published. A positive answer provides a “low risk” score. A package with no named maintainer is scored as high. The rationale behind this metric lies in the fact that the indication of name and email is evidence of a package’s active maintenance and availability of contact in case of bugs and/or suggestions.\nNumber of dependencies (nr_dependencies): This metric assesses the level of risk associated with the number of dependencies a package relies on. Dependencies are other packages or processes that the evaluated package depends on, as listed in CRAN under “depends” and/or “imports”. The risk of unexpected behaviour increases with the number of dependencies, since there is a greater likelihood of issues on a specific routine if updates are performed on a dependent package. Great care should be considered while using packages involving many dependencies.\nThe listed number is converted into a [0, 1] score, with 0 representing low number of dependencies (= low risk) and 1 representing many dependencies (= high risk). Taking a similar approach for the transforming the number to a score as the ‘riskmetric’ package (R validation hub, 2023), we use a simplification of the classic logistic curve \\(1/\\ (1 + exp( - k\\left( x - x\\lbrack 0\\rbrack \\right))\\) as a scoring function. A sigmoid midpoint is 4 dependencies, ie., x[0] = 4 and logistic growth rate of k = 0.5.\n\n\n\n\nA graph with a line Description automatically generated\n\n\n\nCRAN or Bioconductor (on_cran): CRAN and Bioconductor provide a procedure for evaluating the appropriateness and minimal requirements for a package to be released on their official platform. Having passed the tests implemented should guarantee, despite not statistical, a minimal quality. This metric assesses whether the package is on CRAN or Bioconductor. Being on CRAN/Bioconductor provides low risk (yes = 0) while not being on them represents high risk (no = 1).\nDocumentation of source code (source_code_documented): Ideally, the source code is available (for example, on github) for examination and commented (yes = 0). Source code that is not commented or difficult to follow, or is not available, is considered not documented (no = 1).\nNumber of downloads in the last year (nr_downloads_12_months): More downloads suggest more extensive community and user testing and greater chances of bugs or errors being identified and reported. To fill in check the logs of CRAN’s or Bioconductor’s reporting systems and report the number of downloads for the package in the last 12 months. Using the cranlogs::cran_downloads() function to this end is acceptable.\nThe number of downloaded packages is converted to a score [0,1], with 0 representing low risk (many downloads) and 1 high risk (few downloads). For the conversion of the number into a score we follow a similar approach as approach taken by the ‘riskmetric’ package (R validation hub, 2023) and use a simplification of the classic logistic curve \\(1/\\ (1 + exp( - k\\left( x - x\\lbrack 0\\rbrack \\right))\\) with the logistic growth rate k = 0.5 and a log-scale for the number of downloads (log(x)). The midpoint lying at log(100,000 downloads).\n\n\n\n\nA graph with a blue line Description automatically generated\n\n\n\nBug reporting (bug_reporting_active): Available option for reporting bugs suggests higher chance that errors have been corrected. Check whether there is an option to report bugs – ideally via url or email. Yes = 0 (low risk); no = 1 (high risk).\nVignettes (has_vignettes): Does the package have one/more available vignettes? Vignettes provide an explanation of the use of the package, increasing its trustworthiness and correct use. Having at least one vignette suggests a “low” risk for this category (yes = 0); having no vignettes suggests a “high” risk score (no = 1).\nTested functions (has_tests): Perform a search and give a general grade of low/medium/high based on the answers to the following questions: does the package have unit and/or function tests performed by the authors? are they comprehensive? are they well documented? Assess the above to determine whether tests were conducted sufficiently and documented. Accordingly, you can classify into low risk (=0); insufficient testing could be medium risk (=0.5); no documented testing at all are categorized as high risk (=1).\nNote that test functions are often listed in ‘test’ folder in the package’s source files, e.g., on github.\n\n\n\nCalculation of the final risk score:\nThe final score is a weighted summary of all the measures above in the range [0, 1], with lower scores representing lower risk and higher scores higher risk. In this version of the guideline all measures are considered equally important, and the score is a simple arithmetic mean of the measures.\nThe [0, 1] score is then categorized to low, medium and high-risk packages:\n\nScore ≤0.25: low risk\n0.25 &lt; score ≤ 0.75: medium risk\nScore &gt;0.75: high risk\n\nAs described in the SCTO validation policy, the risk associated with the package is evaluated alongside the risk associated with a specific project to determine which actions may be required in order to use the package. High risk packages, or medium risk packages in high-risk projects, for example, may need (project-) specific function testing.\n\n\nDocumentation\nThe assessment of risk associated with a package should be documented and may be updated with time.\nThe SCTO has developed a platform on which the risk assessment can be performed (https://github.com/SwissClinicalTrialOrganisation/pkg_validation). Apart from calculating the risk based on the above metrics, the platform also records the assessment itself for documentation and further use. The meta-data recorded, apart from the metrics themselves, is listed in Appendix 2.\n\n\nReferences\n\nR Validation Hub, Kelkhoff D, Gotti M, Miller E, K K, Zhang Y, Milliman E, Manitz J (2023). _riskmetric: Risk Metrics to Evaluating R Packages_. R package version 0.2.3, &lt;https://CRAN.R-project.org/package=riskmetric&gt;.\n\n\n\nAppendix 1 = The SCTO R package risk metrics\n\n\n\n\n\n\n\n\n\n­­-­­­­‑\nexplanation\npossible values\nrisk level*\n\n\n\n\npackage_name\nname of the package as called for installation\n\n\n\n\nversion\nVersion number of the package evaluated\n\n\n\n\nrelease_date\nDate of release of the current evaluated version of the package\n\n\n\n\nstatistical_package\nstatistical packages implement statistical or machine learning algorithms. Non-statistical packages are used for reporting for example\nnon-statistical / statistical-published / statistical non-published\nlow / med / high\n\n\nauthor\nname of the main author or developing group\nwell-known or known credentials / has credentials / no clear credentials or group association\nlow / med / high\n\n\nmaintainer\nIs there a maintainer listed for the package and are their contact details available?\nYes / No\nlow/high\n\n\nnr_dependencies\nThe more packages a package depends on the more chance for errors/bugs to be found\nNr of dependencies (transformed to [0-1])\n0-1\n\n\non_cran\nis the package on available from CRAN or bioconductor\nYes / No\nlow / high\n\n\nsource_code_documented\nis source code available, accessible and documented (i.e., well-structured and including comments) or is the source code unavailable or not clearly commented.\nYes / No\nlow / high\n\n\nnr_downloads_12_months\nChecked using the CRAN logs package’s implementation– potentially the CRAN (/Bioconductor) reporting system\nNr of downloads (transformed to [0-1])\n0-1\n\n\nbug_reporting_active\naddress for bug reporting exists\nYes / No\nlow / high\n\n\nhas_vignettes\ndoes the package have one/more vignettes\nYes / No\nlow / high\n\n\nhas_tests\ndoes the package have unit and/or function tests performed by the authors? are they comprehensive? are they well documented?\nyes-comprehensive / yes-not-comprehensive / no\nlow / med / high\n\n\nfinal_risk\na global risk, based on and weighing in all the grading metrics. At version one, the weight of all metrics is the same, and the global risk is the average of all metrics.\n[0 – 1] with 0 = low risk and 1 = high risk. Cutpoints categorize into low / med / high\nLow / med / high\n\n\n\n\n* Risk level is always a value [0, 1]. For binary metrics low = 0 and high = 1. ‘medium risk’ takes a value of 0.5. Continuous metrics such as ‘nr of downloads’ are transformed to a value [0, 1].\n\n\n\nAppendix 2 = The SCTO R package documentation\n\n\n\n\n\n\n\n\n------------------------\nExplanation\n\n\n\n\n\npackage_name\nname of the package as called for installation\n\n\n\nversion\nVersion number of the package evaluated\n\n\n\nrelease_date\nDate of release of the current package\n\n\n\ndate_of_risk_assessment\nThe date on which the package risk was performed. Testing probably happens after.\n\n\n\nfinal_risk\nAs calculated from the SCTO risk measures (table above)\n\n\n\nlicense\ndoes the package have a license speciﬁcation?\npackage license speciﬁcation\n\n\ntested_functions\nwhich functions of the package were tested by an SCTO (or associated organizations) member\nlist of functions tested\n\n\nall_tests_passed\nall the tests defined as necessary by the SCTO (or associated organizations) assessor have been performed and passed\nyes / no\n\n\ntesting_date\nDate on which all tests have been completed\n\n\n\ntest_documentation_location\nwhere are the testing procedures and results saved\npath to ﬁles\n\n\nassessor_name\nWho did the risk assessment\n\n\n\ncomments\nfor example, speciﬁc functions that are not recommended or speciﬁc issues"
  },
  {
    "objectID": "results/pkgtesting.html",
    "href": "results/pkgtesting.html",
    "title": "Package tests",
    "section": "",
    "text": "12 packages or package versions have been tested by the SCTO Statistics & Methodology Platform so far.",
    "crumbs": [
      "Package tests"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SCTO Validation Policy",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\n\n\n\n\nName\nDate\n\n\n\n\nAuthored/Revised by\n\n\n\n\nReviewed by\n\n\n\n\nReleased by\n\n\n\n\n\nVersion History\n\n\n\nVersion\nDate\nAuthor\nSummary of Changes\n\n\n\n\n0.1\n2024-04-18\nAlan Haynes\nInitial draft",
    "crumbs": [
      "SCTO Validation Policy"
    ]
  },
  {
    "objectID": "index.html#sec-in-scope",
    "href": "index.html#sec-in-scope",
    "title": "SCTO Validation Policy",
    "section": "In Scope",
    "text": "In Scope\n\nRecommendations for R base installation validation (the final process and all documentation has to be defined and prepared by each CTU according to their local processes)\nProcesses for R add-on \"intended for use” packages validation (incl. package risk assessment & functional testing)\n\nProcesses for R add-on package validation described in this document are to be \nconsidered minimum requirements as shared by the SCTO platform. Individual \norganizations (CTUs) can define stricter processes.",
    "crumbs": [
      "SCTO Validation Policy"
    ]
  },
  {
    "objectID": "index.html#sec-out-of-scope",
    "href": "index.html#sec-out-of-scope",
    "title": "SCTO Validation Policy",
    "section": "Out of Scope",
    "text": "Out of Scope\n\nStandards for IT infrastructure (to be defined on local level by the CTUs, see Section 5)\nR add-on package management to ensure traceability and reproducibility on R product level (to be defined on local level by the CTUs, see Section 5.3)\nRisk assessment and management of R products (to be defined on local level by the CTUs, see Section 5.4): Products are always CTU and clinical trial specific – a high-risk product may result in the need for additional validation activities for an R package, even if that package/ required function is available as “validated” on the SCTO platform.\nIn general: any additional internal, organization specific, processes of CTUs.",
    "crumbs": [
      "SCTO Validation Policy"
    ]
  },
  {
    "objectID": "index.html#sec-it-inf",
    "href": "index.html#sec-it-inf",
    "title": "SCTO Validation Policy",
    "section": "IT Infrastructure Qualification",
    "text": "IT Infrastructure Qualification\nThe IT infrastructure qualification is managed at CTU level according to local IT processes. The risk associated with IT infrastructure components is usually low.\nFor the reliable use of R and statistical traceability it is important to note which IT infrastructure (specifically which operating system software and version) R is installed on and to ensure that the infrastructure on which R is running is qualified.",
    "crumbs": [
      "SCTO Validation Policy"
    ]
  },
  {
    "objectID": "index.html#sec-r-base-installation-validation",
    "href": "index.html#sec-r-base-installation-validation",
    "title": "SCTO Validation Policy",
    "section": "R Base Installation Validation",
    "text": "R Base Installation Validation\nEvery CTU must define:\n\nThe minimal (validated) software version of the R Base Installation to be used at the CTU and a process of handling version changes.\nThe R base installation validation package documenting the validation activities (see table below for recommendations).\n\nThe R base installation validation package is a set of documents that should be prepared to document the validation process. The following table lists all documentation that is typically prepared during a computerised system validation process according to global standards and guidelines. It additionally provides recommendations how this documentation may be covered when preparing an R base installation validation package. The exact content of the R base installation validation package (i.e., the required documentation) at a CTU should follow the local processes and all documentation must be prepared and approved by the CTU. For some of the documents listed below, the SCTO provides examples (as indicated under “Detailed Description”).\n\n\n\n\n\n\n\nDocument\nDetailed Description\n\n\n\n\nR Base installation High-Level Risk Assessment (HLRA; also known as System Risk Assessment, SRA)\nThe HLRA is usually created once, with the first validation of an R base installation version and is valid for all future validated versions. It must be updated in case the scope/ intended use/ content of the software changes.\nThe HLRA usually also documents the GxP relevance of the system and therefore the need to validate the system.\n\nExample available on the SCTO platform\n\n\n\nVendor Assessment\nVendor assessments are usually conducted with the first validation of a software version. Since R is an open-source software managed by a consortium, a traditional vendor assessment/ audit approach is not feasible. A critical review of “R: Regulatory Compliance and Validation Issues - A Guidance Document for the Use of R in Regulated Clinical Trial Environments” may suffice as vendor assessment and should be documented together with any gaps identified that may require attention during the validation of R and its packages.\nThe assessment of this document should be repeated with a publication of a new article version, or in case the R consortium replaces the existing document, or periodically according to each CTU’s local periodic review processes.\n\nExample available on the SCTO platform\n\n\n\nValidation Plan & Test Plan\nThe Validation Plan describes the details of the planned validation process, including required documentation and acceptance criteria for productive use.\nThe Test Plan describes the testing strategy and testing process and may contain details on the tests to be executed.\nThe exact content of both documents depends on the CTU’s local processes. The content of the two documents may also be covered in one “Validation & Test Plan”.\n\nExample available on the SCTO platform\n\n\n\n(User) Requirements Specification\nThis document is usually created for the first validation of an R base installation version and should be reviewed, and if required updated, for future validated versions.\nIt specifies the user requirements and intended use for the R base installation. It may also contain any compliance (e.g., data integrity), regulatory (e.g., personal data protection) and safety requirements (e.g., controlled access).\n\nExample available on the SCTO platform\n\n\n\nBase installation (Functional) Risk Assessment\nThis document is usually created with the first validation of an R base installation version based on the user requirements or system functions supporting these requirements and should be reviewed and, if required, updated for future validated version.\n\nExample available on the SCTO platform\n\n\n\nSoftware installation plan/ instruction\nTypically, either the vendor or the local IT department provides a software installation plan/ instructions to follow when installing new software.\nIn case of R, a specific “R software installation plan/ instruction” is not required. It should be sufficient to follow the instructions on CRAN when downloading and installing the new R base installation version.\nImportant is that the installation of the R base installation and version management follow the CTU’s local processes for validated systems and that all versions installed are documented, including the date/ time of installation (see Installation Verification Document below).\n\n\nTest scripts (“User Acceptance Tests”)\nTest scripts are usually created with the first validated R base installation version and should be reviewed, and if required updated and/or new test scripts created, for future validated version.\nThe tests must verify the functions of the R base installation supporting the user requirements. Test scripts provided by R may be used as-is or adapted/ extended as required. The scope of testing must be defined according to the CTU’s local processes and defined user requirements.\n\nExample available on the SCTO platform\n\n\n\nInstallation Verification document (TEST)\nThe purpose of this document is to provide evidence of the installation of a specific system version on an environment/ machine that is used for testing purposes before the new system version is released to production.\nIn case of R this may be a separate document or an entry in an R repository or other “version tracker” (depending on your local IT processes). The following information should be captured:\n\nR base installation version installed\nDate and time of installation\nPerson performing the installation (may be someone from your local IT department)\nOutcome of installation: successful, successful with deviations, not successful\nIf applicable: Any actions taken in addition to the regular “download and install from CRAN” process (e.g., uninstalling an older version)\n\nNote: depending on your local processes it may not be feasible or even possible to do a preliminary “test installation” and perform tests before productive use of the new R base installation version (e.g., if R is distributed remotely by your local IT department). In this case, we recommend you still execute the tests after the installation and document the test execution. Should any issues be detected during testing, please inform your local IT department immediately. Ideally you implement a process that would ensure nobody is using the new R version for productive use before the tests are completed successfully (i.e., without major issues) and the validation & test report is approved (see “Validation & Test Report” below).\n\n\nExecuted test scripts (“User Acceptance Tests”)\nExecute the pre-defined test scripts for each new R base installation version (i.e., follow the steps described in the test scripts) and document the outcome according to the CTU’s local processes.\n\nExample available on the SCTO platform\n\n\n\nTraceability Matrix\nTraceability between requirements and tests can be achieved by creating a separate document (traceability matrix) or by linking requirements and tests within the requirements and tests themselves. It is important, that whatever means is used, it is feasible to proof that all requirements are verified with a test (or if not tested formally, a rationale why the test is not required, e.g., low risk requirements which can be considered “verified” based on the experience by the user community).\n\n\nTest Report and Validation Report\nThe Test Report and Validation Report are usually created (or at least updated to a new version) with every new R base installation version to be released for productive use. The main purpose is\n\nto summarize the testing activities and results, including any deviations from the expected results\nto document all completed validation activities, including any deviations from the original plan (see Validation & Test Plan). Deviations should be justified and assessed for criticality.\nto document the acceptance for productive use (with or without restrictions)\n\nThe exact content of this report/ these reports depends on the CTU’s local processes.\n\nExample available on the SCTO platform\n\n\n\nInstallation Verification document (PROD)\nThe purpose of this document is to provide evidence of the installation of a specific system version on an environment/ machine that is used for productive purposes.\nIn case of R, this may be a separate document or an entry in an R repository or other “version tracker” (depending on your local IT processes). The following information should be captured:\n\nR base installation version installed\nDate and time of installation\nPerson performing the installation (may be someone from your local IT department)\nOutcome of installation: successful, successful with deviations, not successful\nIf applicable: Any actions taken in addition to the regular “download and install from CRAN” process (e.g., uninstalling an older version)\n\nSee also note under “Installation Verification document (TEST)”: In case of R base installation, the installation may only be documented once and not separately on “TEST” and “PROD”. Important is, that any version changes of the R base installation are managed and documented according to your CTU’s local IT change management processes.",
    "crumbs": [
      "SCTO Validation Policy"
    ]
  },
  {
    "objectID": "index.html#sec-pkgvalidation",
    "href": "index.html#sec-pkgvalidation",
    "title": "SCTO Validation Policy",
    "section": "R Add-on Package Validation",
    "text": "R Add-on Package Validation\nThe R Add-on Package validation package is a set of documents that should be prepared for the documentation of the validation process of an R add-on package following this policy and associated SOPs. The following table lists all documentation that is typically prepared during a computerised system validation process. It additionally provides details on how this documentation is covered for an R add-on package validation according to this policy.\n\n\n\n\n\n\n\n\nDocument/ Activity\nDetailed Description\n\n\n\n\n\nValidation & Test Plan\nNo separate document is created.\nRationale: The content typically contained in a Validation & Test Plan is determined, with respect to R add on packages, in the various steps of this policy and the associated SOPs and its documentation is specified therein as follows:\n\nDetermine the product associated risk (local CTU SOP)\nDetermine the package associated risk (based on SCTO Risk Metrics)\nDetermine, if testing is required (see\n\n\n\nPerform and document testing, if required (SCTO Function Testing SOP)\nDocument compliance with this policy and above-mentioned SOPs (local CTU process, e.g., in metadata or in a document)\n\n\n\n\nPackage High-Level Risk Assessment\nThe add-on package risk depends on factors described in the SCTO Risk metrics.\nThe R add-on package high-level risk assessment should be documented according to the SCTO R-Package Risk Metrics on the SCTO designated platform.\n\n\n\n(User) Requirements Specification\nNo dedicated (User) Requirements document is created.\nRationale: The requirements and intended use of an R package and its functions can only be defined in relation to a product and the required outcome (“intended use”) of a specific function for that product. Therefore, the requirements can only be documented when planning the R product.\nRequirements shall be documented in the Statistical Analysis Plan or a Statistical Protocol or in other, similar documentation4. It is important that this “plan” indicates the intended use and what is required of the functions planned to be used within the R package.\n\n\n\nFunction Risk Assessment\nThe risk of separate functions within a package can | only be assessed in relation to a product and the | required outcome (“intended use”) of a specific | function for that product (see 1).\n\n\nPackage installation plan/ instruction\nThere is no need for a separate installation plan or instruction. The installation follows a standard procedure using R base installation functions.\nOne of the following procedures is followed:\n\nR add-on packages may be installed for functional testing after the high-level package risk and the product related requirements and functional risk are documented and the testing is planned, if the initial assessment shows that the required package is not yet sufficiently tested by one of the SCTO members providing input to the R add-on validation package repository.\nR add-on packages may be installed for productive use/ used to create R products after the functional tests are completed and no deviations were detected that would raise concerns against using the tested package functions for a given R product OR if the initial assessment shows that the required package was already sufficiently tested and the respective validation package is available in the SCTO R add-on package validation repository.\n\n\n\n\nTest scripts\nThe process of writing and executing tests for the R add-on packages is covered in the SCTO Function Testing SOP.\n\n\n\nSoftware installation verification (TEST)\nNo separate document is created.\nRationale:\nThe purpose of this document would be to provide evidence of the installation of a specific add-on package (version) for testing purposes. In the context of R add-on package validation it is not feasible to create a separate document. The tested R add-on package (version) functions are documented when following the SCTO Function Testing SOP.\n\n\n\nExecuted test scripts\nExecute the pre-defined test scripts and document the outcome of each step as well as the overall outcome of the test script (see SCTO Function Testing SOP).\n\n\n\nValidation & Test Report\nThe purpose of this document is to summarize all executed validation activities and their outcome, including testing and any defects found and/or any deviations from the original plan with a rationale. It documents the decision, if specific functions from an R add-on package (version) can be accepted for productive use or not and if there are any restrictions, specifies these restrictions (see SCTO Function Testing SOP).\n\n\n\nSoftware installation verification (PROD)\nNo separate document is created.\nRationale:\nThe purpose of this document would be to provide evidence of the installation of a specific add-on package (version) for productive use. In case of an R add-on package and its functions the “productive use” is determined by the product upon calling library(). Therefore, it is not feasible to create a separate installation verification document.\nWhat needs to be ensured when using a specific function within an R add-on package:\n\nDocumentation of package risk assessment based on the SCTO risk metrics, and of function testing according to SCTO Function Testing SOP\nDocumentation of package use (meta-data/session info)",
    "crumbs": [
      "SCTO Validation Policy"
    ]
  },
  {
    "objectID": "index.html#sec-r-product-validation",
    "href": "index.html#sec-r-product-validation",
    "title": "SCTO Validation Policy",
    "section": "R Product Validation",
    "text": "R Product Validation\nTo be covered by local SOPs for statistical analysis execution at the CTUs. The R product validation should cover at minimum:\n\nAssessment of risk associated with the R product\nAssess the tools used within the R product\nDocumentation for reproducibility and traceability",
    "crumbs": [
      "SCTO Validation Policy"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "SCTO Validation Policy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nfor purposes of this document referred to as CTUs = Clinical Trial Units↩︎\nA validation package is a set of documents prepared for the documentation of the validation process itself.↩︎\nFor example, standards set by the local IT department.↩︎\nUser requirements for sample size estimation or power analyses are inherently the sample size or the power or a combination of both and do not need to be documented in a separate plan.↩︎",
    "crumbs": [
      "SCTO Validation Policy"
    ]
  },
  {
    "objectID": "results/pkgassessment.html",
    "href": "results/pkgassessment.html",
    "title": "Package assessments",
    "section": "",
    "text": "20 packages or package versions have been risk assessed within the SCTO Statistics & Methodology Platform so far.\n\n\n\n\n\n\n\n\n\nThe following packages have newer versions available, which may be an indication that the risk assessment is outdated:\n\n\n# A tibble: 0 × 3\n# ℹ 3 variables: Package &lt;chr&gt;, Validated version &lt;chr&gt;,\n#   Available version &lt;chr&gt;",
    "crumbs": [
      "Package assessments"
    ]
  },
  {
    "objectID": "sop/highlevel_ra.html",
    "href": "sop/highlevel_ra.html",
    "title": "High level risk assessment",
    "section": "",
    "text": "This is an explanation and ‘dictionary’ for the High Level Risk Assessment tool of the platform.\nThe file is structured as a process following the steps below:\n\nThe variables of decision are:\n\n\nRisk area:\nRisk Subarea\nImpact of risk (A):\nMinor (1 point)\nMajor (3 points)\nCritical (6 points)\nLikelihood (A): how likely is the event to take place (before mitigating actions are in place):\nUnlikely (1 points)\nPossible (2 points)\nLikely (3 points)\nRisk A = multiplication of impact x likelihood\nRisk A category: categorization of Risk according to points received in Risk A\n1-2: Low (green)\n3-8: Medium (yellow)\n≥9 High (red)\nDetectability A: how fast and easily is the risk effect detected, potentially before consequences (before mitigating actions are in place):\nHigh (easy to detect, ‘jumps’ to the eye immediately)\nMedium (is detectable if one pays attention or examines this point specifically with a critical eye)\nLow (hard to detect, only detectable if rigorously and specifically looking for problems)\nPriority A: The ‘final’ risk category and priority in need for handling (i.e., before mitigating actions). Is conditional on the Risk-A category and on the Detectability A according to the PharmaSUG suggestion (Figure 2) using for “risk class” the defined Risk Category A\n\n\n\n\nRisk treatment: the mitigating actions that can be implemented. Examples of possibilities are provided. Mitigating actions may:\nReduce the likelihood of a risk to occur\nIncrease the likelihood of detection of the risk occurring\n\n\n\n\nRisk analysis after treatment:\nImpact = the same as the impact in the risk assessment (does not change)\nLikelihood B: The likelihood of the risk to happen (Once mitigating actions are in place)\nRisk B: the calculated risk based on impact and the new likelihood\nDetectability B: the new detectability (Once mitigating actions are in place)\nPriority B: Final, residual, priority of the risk (once mitigating actions are in place)\n\n\n\n\nRisk monitoring: TO BE FILLED per CTU according to local SOPs and guidelines (local QM)\n\n\nThe high level risk assessment is available here.",
    "crumbs": [
      "High level risk assessment"
    ]
  },
  {
    "objectID": "sop/risk_metrics.html",
    "href": "sop/risk_metrics.html",
    "title": "Risk metrics",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\n\n\n\n\nName\nDate\n\n\n\n\nAuthored/Revised by\n\n\n\n\nReviewed by\n\n\n\n\nReleased by\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nPosition\nOrg anization\nAddress\nemail\n\n\n\n\nMichael Coslovsky, PhD\nHead da ta-analysis\nD epartment Clinical Research, U niversity of Basel\nSpit alstrasse 12, 4031, Basel\nMichael.c oslovsky@usb.ch\n\n\nNicole Graf\nS tatistician\nClinical Trials Unit, Kant onsspital St. Gallen\nHaus 89,\nRorschach erstrasse 95, 9007 St. Gallen\nNico le.graf@kssg.ch\n\n\nJulien Sauser\nS tatistician\nCHUV\n\nJulien .sauser@chuv.ch\n\n\nChristina Huf\nHead Quality Management\nD epartment Clinical Research, U niversity of Bern\n\nchristi na.huf@unibe.ch\n\n\nElio Carreras\nSenior Statistical Programmer\nSAKK\n\nElio.C arreras@sakk.ch\n\n\n\nVersion History\n\n\n\nVersion\nDate\nAuthor\nSummary of Changes\n\n\n\n\n0.1\n2024-04-18\nAlan Haynes\nInitial draft\n\n\n\n\n\n\n\nObjective\nThis document describes the metrics used by members of the Swiss Clinical Trial Organization when evaluating add on packages of the statistical environment, R. It provides a table of the metrics and a short explanation of the considerations behind each of the metrics.\nAssessing the risk associated with using a software package is one of the steps required for working under a validated environment, as determined in the SCTO R-validation policy. The risk of (using) a package is the opposite of the confidence we have in the package’s delivered output: packages which we are confident in using are of low risk, and vice versa. The metrics described below are the building stones in determining this risk. A final risk score is determined based on a weighting scheme including these metrics.\nFollowing this assessment, the assessor determines whether any parts of the package need to be tested (function/unit testing) for the package’s intended use in a specific product. The documentation of any actions following the initial risk assessment is listed in a separate table.\nThis document is part of the SCTO Computerized Systems Validation Policy for R (NAME – LOCATION – VERSION) and refers to the standard operation procedure for Function testing (NAME – LOCATION) of the SCTO statistics platform.\nThe metrics are written with the aim of assessing R packages.\nBased on the principals below, the table in the addendum provides the metrics’ values to be documented upon package evaluation for the SCTO platform statistics. Note that the evaluation is per package with a specific version number. Package version changes and updates require re-assessment of the metrics for the new version.\n\n\nSCOPE\nThis document is applicable to all R add-on packages. Out of scope are packages included in the official R-distribution including “Base R” packages. Also out of scope is the collection of “recommended packages”, developed and validated by members of the R Development Core Team, as listed in the document “R: Regulatory Compliance and Validation Issues, The R Foundation for Statistical Computing, 2021”. R-base and recommended packages are approved per definition, as stated in the policy document.\n\n\nMETRIC EXPLANATION - glossary\nThe following metrics are considered when assessing a package. These are listed in Appendix-1 below. Here we provide a precise definition:\n\nPackage name, version, release date: version and release date of the specific version assessed at this moment. These are required for identifying the package and for documentation. Packages will be re-assessed upon update and version changes; documentation of older versions remains in the document. These values are not included in the risk calculation.\nPurpose (statistical_package) We define three risk levels for a package, depending on the package’s purpose and methodology:\n\n“non-statistical” packages: packages that deal only with data-wrangling and manipulation (e.g., dplyr) or with reporting processes (e.g., Sweave, xtable). Such processes are of ‘low risk’ as no statistical calculations are performed, and data-errors are, comparatively, easy to detect. Similarly, packages associated with application interfaces such as Shiny application are considered “non-statistical”.\n“Statistical with published methods”: packages that perform statistical calculations based on known methods, or on methods that have been published in peer reviewed journals. These packages obtain a “medium risk” status.\n“Statistical non-published methods”: packages that perform statistical calculations, but the underlying methods have not been published in a peer reviewed journal. These packages obtain a status “high risk”.\n\nAuthor (author): The author(s) of a package will be viewed as indicator for its trustworthiness. If package authors (noted as ‘aut’ in the package description, e.g., as listed on CRAN) are well-known within the statistical, data-science and R communities and have credentials based on their qualifications, education, present and past affiliations, the risk of the respective package will be scored as low. If package authors have credentials based on their qualifications, education, present and past affiliations, but are not well-known within the statistical, data-science and R communities, the risk of the respective package will be scored as medium. If package authors are not well-known within the statistical, data-science and R communities and have no clear credentials based on their qualifications, education, present and past affiliations, the risk of the respective package will be scored as high.\nNote that whether an author is ‘well-known’ in the community is a subjective assessment and accepted as such; in addition, groups of authors are evaluated as a collective.\nMaintainer (maintainer): The package has a named maintainer who’s contact details (email) are available and published. A positive answer provides a “low risk” score. A package with no named maintainer is scored as high. The rationale behind this metric lies in the fact that the indication of name and email is evidence of a package’s active maintenance and availability of contact in case of bugs and/or suggestions.\nNumber of dependencies (nr_dependencies): This metric assesses the level of risk associated with the number of dependencies a package relies on. Dependencies are other packages or processes that the evaluated package depends on, as listed in CRAN under “depends” and/or “imports”. The risk of unexpected behaviour increases with the number of dependencies, since there is a greater likelihood of issues on a specific routine if updates are performed on a dependent package. Great care should be considered while using packages involving many dependencies.\nThe listed number is converted into a [0, 1] score, with 0 representing low number of dependencies (= low risk) and 1 representing many dependencies (= high risk). Taking a similar approach for the transforming the number to a score as the ‘riskmetric’ package (R validation hub, 2023), we use a simplification of the classic logistic curve \\(1/\\ (1 + exp( - k\\left( x - x\\lbrack 0\\rbrack \\right))\\) as a scoring function. A sigmoid midpoint is 4 dependencies, ie., x[0] = 4 and logistic growth rate of k = 0.5.\n\n\n\nWarning: package 'validation' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n\n\n\n\n\n\n\n\nCRAN or Bioconductor (on_cran): CRAN and Bioconductor provide a procedure for evaluating the appropriateness and minimal requirements for a package to be released on their official platform. Having passed the tests implemented should guarantee, despite not statistical, a minimal quality. This metric assesses whether the package is on CRAN or Bioconductor. Being on CRAN/Bioconductor provides low risk (yes = 0) while not being on them represents high risk (no = 1).\nDocumentation of source code (source_code_documented): Ideally, the source code is available (for example, on github) for examination and commented (yes = 0). Source code that is not commented or difficult to follow, or is not available, is considered not documented (no = 1).\nNumber of downloads in the last year (nr_downloads_12_months): More downloads suggest more extensive community and user testing and greater chances of bugs or errors being identified and reported. To fill in check the logs of CRAN’s or Bioconductor’s reporting systems and report the number of downloads for the package in the last 12 months. Using the cranlogs::cran_downloads() function to this end is acceptable.\nThe number of downloaded packages is converted to a score [0,1], with 0 representing low risk (many downloads) and 1 high risk (few downloads). For the conversion of the number into a score we follow a similar approach as approach taken by the ‘riskmetric’ package (R validation hub, 2023) and use a simplification of the classic logistic curve \\(1/\\ (1 + exp( - k\\left( x - x\\lbrack 0\\rbrack \\right))\\) with the logistic growth rate k = 0.5 and a log-scale for the number of downloads (log(x)). The midpoint lying at log(100,000 downloads).\n\n\n\n\n\n\n\n\n\n\n\nBug reporting (bug_reporting_active): Available option for reporting bugs suggests higher chance that errors have been corrected. Check whether there is an option to report bugs – ideally via url or email. Yes = 0 (low risk); no = 1 (high risk).\nVignettes (has_vignettes): Does the package have one/more available vignettes? Vignettes provide an explanation of the use of the package, increasing its trustworthiness and correct use. Having at least one vignette suggests a “low” risk for this category (yes = 0); having no vignettes suggests a “high” risk score (no = 1).\nTested functions (has_tests): Perform a search and give a general grade of low/medium/high based on the answers to the following questions: does the package have unit and/or function tests performed by the authors? are they comprehensive? are they well documented? Assess the above to determine whether tests were conducted sufficiently and documented. Accordingly, you can classify into low risk (=0); insufficient testing could be medium risk (=0.5); no documented testing at all are categorized as high risk (=1).\nNote that test functions are often listed in ‘test’ folder in the package’s source files, e.g., on github.\n\n\n\nCalculation of the final risk score:\nThe final score is a weighted summary of all the measures above in the range [0, 1], with lower scores representing lower risk and higher scores higher risk. In this version of the guideline all measures are considered equally important, and the score is a simple arithmetic mean of the measures.\nThe [0, 1] score is then categorized to low, medium and high-risk packages:\n\nScore ≤0.25: low risk\n0.25 &lt; score ≤ 0.75: medium risk\nScore &gt;0.75: high risk\n\nAs described in the SCTO validation policy, the risk associated with the package is evaluated alongside the risk associated with a specific project to determine which actions may be required in order to use the package. High risk packages, or medium risk packages in high-risk projects, for example, may need (project-) specific function testing.\n\n\nDocumentation\nThe assessment of risk associated with a package should be documented and may be updated with time.\nThe SCTO has developed a platform on which the risk assessment can be performed (https://github.com/SwissClinicalTrialOrganisation/pkg_validation). Apart from calculating the risk based on the above metrics, the platform also records the assessment itself for documentation and further use. The meta-data recorded, apart from the metrics themselves, is listed in Appendix 2.\n\n\nReferences\n\nR Validation Hub, Kelkhoff D, Gotti M, Miller E, K K, Zhang Y, Milliman E, Manitz J (2023). _riskmetric: Risk Metrics to Evaluating R Packages_. R package version 0.2.3, &lt;https://CRAN.R-project.org/package=riskmetric&gt;.\n\n\n\nAppendix 1. The SCTO R package risk metrics\n\n\n\n\n\n\n\n\n\n­­-­­­­‑\nexplanation\npossible values\nrisk level*\n\n\n\n\npackage_name\nname of the package as called for installation\n\n\n\n\nversion\nVersion number of the package evaluated\n\n\n\n\nrelease_date\nDate of release of the current evaluated version of the package\n\n\n\n\nstatistical_package\nstatistical packages implement statistical or machine learning algorithms. Non-statistical packages are used for reporting for example\nnon-statistical / statistical-published / statistical non-published\nlow / med / high\n\n\nauthor\nname of the main author or developing group\nwell-known or known credentials / has credentials / no clear credentials or group association\nlow / med / high\n\n\nmaintainer\nIs there a maintainer listed for the package and are their contact details available?\nYes / No\nlow/high\n\n\nnr_dependencies\nThe more packages a package depends on the more chance for errors/bugs to be found\nNr of dependencies (transformed to [0-1])\n0-1\n\n\non_cran\nis the package on available from CRAN or bioconductor\nYes / No\nlow / high\n\n\nsource_code_documented\nis source code available, accessible and documented (i.e., well-structured and including comments) or is the source code unavailable or not clearly commented.\nYes / No\nlow / high\n\n\nnr_downloads_12_months\nChecked using the CRAN logs package’s implementation– potentially the CRAN (/Bioconductor) reporting system\nNr of downloads (transformed to [0-1])\n0-1\n\n\nbug_reporting_active\naddress for bug reporting exists\nYes / No\nlow / high\n\n\nhas_vignettes\ndoes the package have one/more vignettes\nYes / No\nlow / high\n\n\nhas_tests\ndoes the package have unit and/or function tests performed by the authors? are they comprehensive? are they well documented?\nyes-comprehensive / yes-not-comprehensive / no\nlow / med / high\n\n\nfinal_risk\na global risk, based on and weighing in all the grading metrics. At version one, the weight of all metrics is the same, and the global risk is the average of all metrics.\n[0 – 1] with 0 = low risk and 1 = high risk. Cutpoints categorize into low / med / high\nLow / med / high\n\n\n\n\n* Risk level is always a value [0, 1]. For binary metrics low = 0 and high = 1. ‘medium risk’ takes a value of 0.5. Continuous metrics such as ‘nr of downloads’ are transformed to a value [0, 1].\n\n\n\nAppendix 2. The SCTO R package documentation\n\n\n\n\n\n\n\n\n------------------------\nExplanation\n\n\n\n\n\npackage_name\nname of the package as called for installation\n\n\n\nversion\nVersion number of the package evaluated\n\n\n\nrelease_date\nDate of release of the current package\n\n\n\ndate_of_risk_assessment\nThe date on which the package risk was performed. Testing probably happens after.\n\n\n\nfinal_risk\nAs calculated from the SCTO risk measures (table above)\n\n\n\nlicense\ndoes the package have a license speciﬁcation?\npackage license speciﬁcation\n\n\ntested_functions\nwhich functions of the package were tested by an SCTO (or associated organizations) member\nlist of functions tested\n\n\nall_tests_passed\nall the tests defined as necessary by the SCTO (or associated organizations) assessor have been performed and passed\nyes / no\n\n\ntesting_date\nDate on which all tests have been completed\n\n\n\ntest_documentation_location\nwhere are the testing procedures and results saved\npath to ﬁles\n\n\nassessor_name\nWho did the risk assessment\n\n\n\ncomments\nfor example, speciﬁc functions that are not recommended or speciﬁc issues",
    "crumbs": [
      "SOPs",
      "Risk metrics"
    ]
  },
  {
    "objectID": "wi/pkg_validation.html",
    "href": "wi/pkg_validation.html",
    "title": "Risk assessing packages",
    "section": "",
    "text": "This page functions as the standard operating procedure for the risk assessment of R packages within the SCTO Statistics & Methodology Platform framework.\nIn order to perform a risk assessment:\n\nfirst, check whether the package has already been risk assessed. This can be done by any of the following methods:\n\nthe check_session function from the R package validation can be used from within your R session to check if a package has been risk assessed.\ncheck the table on the home page of this site\ncheck for the package in the package validation GitHub repository by searching for it among the issues using the search bar towards the top of the page (remember to remove the is:open filter). E.g. to search for dplyr, the search might be is:issue dplyr\n\nif it has been risk assessed, check the associated risk and consider whether any functions within package need additional testing for your use case.\nif it has not been risk assessed, go to the package validation GitHub repository\n\nclick the green “New issue” button (towards the top right)\nselect the New package validation template by clicking the green Get started button on the right\nfill in the form, following the instructions provided. Additional notes on the risk metrics can be found on the Risk metrics page.\n\nNote that the R package validation contains various useful functions to help with the validation process. Especially useful are:\n\nget_n_deps, which returns the number of dependencies of a package,\nget_12month_downloads which returns the number of downloads of a package in the last 12 months, and\nget_release_date finds the release date of a package.\n\n\nOnce you have completed the form, click the green “Submit new issue” button at the bottom of the page.\nSubmitting the form will trigger an automated job that will calculate the risk based on the information you entered into the form and post a comment on the issue.\n\n\n\n\n\n\n\nflowchart TD\n  A[Identify the package that may need risk assessment] --&gt; B(Check whether the package has already been risk assessed)\n  B --&gt; C1(\"`Within your R session, load the package and use the function \n  &lt;span style=\"font-family:Courier; font-size: 80%\"&gt;validation::check_session()&lt;/span&gt;`\")\n  B --&gt; C2(Check whether the package is listed on the &lt;a href='index.html'&gt;home page&lt;/a&gt;)\n  B --&gt; C3(Check for an issue on the &lt;a href='https://github.com/SwissClinicalTrialOrganisation/pkg_validation/issues'&gt;GitHub&lt;/a&gt;)\n  C1 --&gt; D(RA exists?)\n  C2 --&gt; D\n  C3 --&gt; D\n  D --Yes--&gt; E[The associated risk level of the package]\n  D --No--&gt; F(Perform a risk assessment following the instructions on &lt;a href='https://github.com/SwissClinicalTrialOrganisation/pkg_validation/issues/new/choose'&gt;GitHub&lt;/a&gt;)\n  F --&gt; G(Open a new issue with the 'New package validation' template)\n  G --&gt; H(Fill out the form following the instructions provided)\n  H --&gt; J(Submit the issue and wait for the automated risk assessment)\n  J --&gt; E\n  E --&gt; K(Considering your use case, do you need to test any functions within the package?)\n  K --No--&gt; N(Use the package)\n  K --Yes--&gt; O(Check the validation_tests repository to see if any functions from the package have been tested)\n  O --&gt; P(Tests are available, and they cover your use case)\n  O --&gt; Q(Tests are available, but they do not cover your use case)\n  O --&gt; R(No tests are available)\n  P --&gt; N\n  Q --&gt; S(Write new tests for the functions you need and submit them to the validation_tests repository)\n  R --&gt; S\n  S --&gt; T(Ensure tests pass)\n  T --&gt; N",
    "crumbs": [
      "Work Intructions",
      "Risk assessing packages"
    ]
  },
  {
    "objectID": "wi/running_tests.html",
    "href": "wi/running_tests.html",
    "title": "Running function tests",
    "section": "",
    "text": "Although the unit of testing is the function, for organisational purposes, the tests are grouped by package. This is because the tests are run in the context of the package, and eases the process of running and reporting tests.\nTests are run via the test function in the validation package:\n\nvalidation::test(\"accrualPlot\")\n\nThe function will download the testing files from GitHub, run the tests and format the results for easier copy/pasting into a reporting issue on GitHub.\n\n\nWarning: package 'testthat' was built under R version 4.3.3\n\n\nLoading required package: accrualPlot\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'accrualPlot'\n\n\nInstalling package into 'D:/a/_temp/Library'\n(as 'lib' is unspecified)\n\n\npackage 'accrualPlot' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\runneradmin\\AppData\\Local\\Temp\\RtmpwvQ113\\downloaded_packages\n\n\nWarning: package 'accrualPlot' was built under R version 4.3.3\n\n\nLoading required package: lubridate\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n✔ | F W  S  OK | Context\n\n⠏ |          0 | accrual_create_df                                              \n✔ |          8 | accrual_create_df\n\n⠏ |          0 | summary                                                        \n✔ |         10 | summary\n\n══ Results ═════════════════════════════════════════════════════════════════════\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 18 ]\n\n\nWarning in rm(accrualdemo): object 'accrualdemo' not found\n\n\nError in gh(endpoint = \"/user\", .token = .token, .api_url = .api_url,  : \n  GitHub API error (403): Resource not accessible by integration\nℹ Read more at\n  &lt;https://docs.github.com/rest/users/users#get-the-authenticated-user&gt;\n\n\n## Copy and paste the following output into the indicated sections of a new issue\n\nISSUE NAME: \n[Package test]: accrualPlot version 1.0.7 \n\n### Name \nrunneradmin \n\n### Name of the package you have validated \naccrualPlot \n\n### What version of the package have you validated? \n1.0.7 \n\n### Where was the package from? \nCRAN (R 4.3.3) \n\n### Package repository version reference\nNA \n\n### When was this package tested? \n2024-08-29 \n\n### What was tested? \nTests for package accrualPlot \n - `summary` produces expected results \n - `accrual_create_df` produces expected results \n  \n These tests are primarily for testing the validation infrastructure. accrualPlot \n has extensive tests \n \n\n### Test results \nPASS \n\n### Test output:\n\n\n|file                     |context           |test                     | nb| passed|skipped |error | warning|\n|:------------------------|:-----------------|:------------------------|--:|------:|:-------|:-----|-------:|\n|test-accrual_create_df.R |accrual_create_df |monocentric              |  4|      4|FALSE   |FALSE |       0|\n|test-accrual_create_df.R |accrual_create_df |multicentric             |  4|      4|FALSE   |FALSE |       0|\n|test-summary.R           |summary           |monocentric as expected  |  5|      5|FALSE   |FALSE |       0|\n|test-summary.R           |summary           |multicentric as expected |  5|      5|FALSE   |FALSE |       0|\n\n### SessionInfo:\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] lubridate_1.9.3  testthat_3.2.1.1\n\nloaded via a namespace (and not attached):\n [1] rappdirs_0.3.3    utf8_1.2.4        generics_0.1.3    tidyr_1.3.1      \n [5] stringi_1.8.4     hms_1.1.3         digest_0.6.37     magrittr_2.0.3   \n [9] grid_4.3.0        evaluate_0.24.0   timechange_0.3.0  pkgload_1.4.0    \n[13] fastmap_1.2.0     rprojroot_2.0.4   jsonlite_1.8.8    sessioninfo_1.2.2\n[17] cranlogs_2.1.1    brio_1.1.5        httr_1.4.7        validation_0.3.6 \n[21] purrr_1.0.2       fansi_1.0.6       scales_1.3.0      httr2_1.0.3      \n[25] cli_3.6.3         rlang_1.1.4       crayon_1.5.3      gitcreds_0.1.2   \n[29] munsell_0.5.1     withr_3.0.1       yaml_2.3.10       tools_4.3.0      \n[33] accrualPlot_1.0.7 tzdb_0.4.0        dplyr_1.1.4       colorspace_2.1-1 \n[37] ggplot2_3.5.1     curl_5.2.2        vctrs_0.6.5       R6_2.5.1         \n[41] lifecycle_1.0.4   stringr_1.5.1     htmlwidgets_1.6.4 waldo_0.5.3      \n[45] pkgconfig_2.0.3   desc_1.4.3        gtable_0.3.5      pillar_1.9.0     \n[49] glue_1.7.0        gh_1.4.1          xfun_0.47         tibble_3.2.1     \n[53] tidyselect_1.2.1  knitr_1.48        htmltools_0.5.8.1 rmarkdown_2.28   \n[57] readr_2.1.5       pkgsearch_3.1.3   compiler_4.3.0   \n\n\n### Where is the test code located for these tests?\nSwissClinicalTrialOrganisation/validation_tests\n\n### Where the test code is located in a git repository, add the git commit SHA\na696e6b665e9e8ea761c00d4126e86cc3defc816\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt may be that the package being tested, or indeed functions in the testing infrastructure (testthat and waldo) have been updated since the tests were authored. This may lead to tests failing that previously passed. In this case, the tests should be updated to reflect the new behaviour.\nOne particular case is a change in attributes. For example, the coef method for a model object returns a named vector, but we might compare the individual coeffients to a vector of expected values. There is thus a potential mismatch in the attributes of the two vectors, which may cause the test to fail with a names for target but not for current message. Use of the third edition of testthat (v3.0.0) should help to avoid this issue.",
    "crumbs": [
      "Work Intructions",
      "Running function tests"
    ]
  }
]